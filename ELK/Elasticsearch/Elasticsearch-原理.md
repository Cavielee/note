# 原由

为什么使用 Elasticsearch？Elasticsearch 有什么好处？

## 结构化和非结构化的数据

**结构化数据：** 也称作行数据，是由二维表结构来逻辑表达和实现的数据，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。指具有固定格式或有限长度的数据，如数据库，元数据等。

**非结构化数据：** 又可称为文本数据，不定长或无固定格式，不适于由数据库二维表来表现，包括所有格式的办公文档、XML、HTML、Word 文档，邮件，各类报表、图片和咅频、视频信息等。



对于结构化数据，因为它们具有特定的结构，所以一般都是可以通过关系型数据库（MySQL，Oracle 等）的二维表（Table）的方式存储和搜索，也可以建立索引。

对于非结构化数据，即文本数据，一般使用非关系型数据库的方式存储和搜索，其搜索主要有两种方法：

* **顺序扫描：** 按照顺序扫描的方式查询特定的关键字。由于需要全文扫描，因此这种方式是最耗时的最低效的。

* **全文搜索：** 将非结构化数据中的一部分信息提取出来，建立索引。



而 Elasticsearch 就是为了解决非结构化的数据进行搜索的引擎（全文搜索）。

其实现实际是依赖其底层 Lucene 进行全文搜索，而 Lucene 能实现全文搜索，主要是靠倒排索引的查询结构。

> 市面上比较成熟的搜索引擎实际还有 Solr，两者功能大致相同，也能支持分布式，其主要区别有两点：
>
> 1. Solr 的分布式协调需要第三方中间件 Zookeeper 去实现，而  ElasticSearch 则本身自实现分布式协调，因此 ElasticSearch 更方便使用。
> 2. 对于频繁更新、数据量大时，Solr 的响应速度和实时性较差。



## 倒排索引

在结构化数据，如Mysql，使用的索引都是使用正向索引。

![image-20220830122120268](https://raw.githubusercontent.com/Cavielee/notePics/main/倒排索引.png)

如果要搜索文档1的内容时，可以从id索引搜索，然后得到对应的文档内容。（正向索引）

但如果要搜索关键词 Java 出现在那些文档（即全文搜索），则需要遍历所有文档，对每个文档的内容进行顺序扫描。可以看出正向去搜索文本内容时，效率会十分低。

对于非结构化数据搜索，如文本数据，因此 Lucene 采用反向索引的概念使用倒排索引。

倒排索引主要原理如下：

![image-20220830142758807](https://raw.githubusercontent.com/Cavielee/notePics/main/倒排索引2.png)

![图片](https://mmbiz.qpic.cn/mmbiz_png/PxMzT0Oibf4hVH3IUycaXbZib5Koq8WmfkoicqcEKGkEaJgoRj9ScrmHQvNN4yp6CXKPvg114OpDian2PSqQibM9MuA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **词条（Term）：**索引里面最小的存储和查询单元，对于英文来说是一个单词，对于中文来说一般指分词后的一个词。Elasticsearch 会将文本内容通过分词器拆分成多个词。
- **词典（Term Dictionary）：** 由文本内容拆分出来的 Term 组成。词典中的每个 Term 存储了单词本身信息和指向 `倒排列表` 的指针。
- **倒排列表（Post list）：** 倒排列表实际上是个链表，链表每一个项称为 `倒排项（Posting）`，倒排项记录了该单词出现在那个文档，在该文档的词频、出现位置和偏移量。 
- **倒排文件（Inverted File）：** 所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件被称之为倒排文件，倒排文件是存储倒排索引的物理文件。

从上图可以看出，倒排索引有两部分组成：

- 词典
- 倒排文件

词典和倒排表是 Lucene 中很重要的两种数据结构，是实现快速检索的重要基石。词典和倒排文件是分两部分存储的，词典在内存中而倒排文件存储在磁盘上。



倒排索引流程：

1. 首先会通过分词器对文本内容进行分词，得到的词我们称为 Term。

2. 创建一个包含所有不重复词条的排序列表（词典），然后列出每个词条出现在哪个文档（倒排列表）。

   ```
   Term          Doc_1    Doc_2   Doc_3  
   -------------------------------------  
   Java        |   X   |        |  
   is          |   X   |   X    |   X  
   the         |   X   |   X    |   X  
   best        |   X   |   X    |   X  
   programming |   x   |   X    |   X  
   language    |   X   |   X    |   X  
   PHP         |       |   X    |  
   Javascript  |       |        |   X  
   -------------------------------------  
   ```

3. 此时当搜索某个关键词时，就不需要对文本内容进行顺序扫描，直接遍历倒排索引查询是否有对应的 Term，然后快速定位倒排列表，遍历找出对应的文档。



# 结构

Elasticsearch 中大致分为四个部分：

* 索引（index）：结构化的存储，如存储什么类型的文档、文档中字段类型、分片配置。可以类比成关系型数据库的 DB。
* 索引类型（indexType）：用于定义索引存储的文档类型，6.x版本后默认只能存储为 `.doc` 类型，即文档。
* 文档（Document）：通过 JSON 格式存储内容，可以类比成关系型数据库中的记录行。

总结：一个 Elasticsearch 集群可以包含多个 **索引** ，相应的每个索引可以包含多个 **类型** 。 这些不同的类型存储着多个 **文档** ，每个文档又有多个 **属性** 。



# 特性

为什么使用 Elasticsearch？

1. Elasticsearch 是使用 Java 编写的一种开源搜索引擎，它在内部使用 Lucene 做索引与搜索，通过对 Lucene 的封装，隐藏了 Lucene 的复杂性，取而代之的提供一套简单一致的 RESTful API。
2. Elasticsearch 是一个分布式、可扩展、近实时的搜索与数据分析引擎。



# 分布式

## 集群

一个 Elasticsearch 服务启动实例就是一个节点（Node）。节点通过 `node.name` 来设置节点名称，如果不设置则在启动时给节点分配一个随机通用唯一标识符作为名称。

Elasticsearch 集群不需要第三方分布式协调组件去维护集群中的节点，其自身内部实现了分布式协调。

集群中的节点只需要配置相同的集群名和集群节点之间的通信地址（ip+端口）即可。



### 发现机制

Zen Discovery 是 Elasticsearch 的内置默认发现模块，其主要作用有两点：

* 发现集群中的节点；
* 选举 Master 节点。



**发现集群中的节点：**

节点可以通过单播或者文件的方式去发现集群中的其他节点。默认使用单播发现，以防止节点无意中加入集群（文件方式会固定集群中的节点）。

Elasticsearch 会通过 Ping 的方式去查找节点，默认会查找同一台机器上运行的节点，并将 `cluster.name` 配置相同的节点自动组成集群。

如果集群的节点运行在不同的机器上且使用单播，可以配置节点列表，Elasticsearch 会去尝试连接节点列表的节点。

当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 Master 节点，并加入集群。因此理论上节点只要连接上集群中的其中一个节点即可，但前提是集群至少有一条链路连接所有节点。为了避免某个节点故障导致无法连接上集群、获取集群其他节点信息，一般节点列表都会提供多个节点。

Elasticsearch 通过 `discovery.seed_hosts` 配置去扫描指定地址，从而获取集群节点列表。（默认地址是 `"127.0.0.1", "[::1]"`）

```yml
# 集群节点列表（实际只需连上集群其中一个节点即可获取整个集群的节点列表）
discovery.seed_hosts: ["127.0.0.1:9300","127.0.0.1:9301","127.0.0.1:9302"]
```

节点启动会依次 ping `discovery.seed_hosts` 设置中的 Host，Ping 的 Response 会包含该节点的基本信息、认为的 Master 节点、Master 候选列表。

> 如果没有配置 `discovery.seed_hosts`、`cluster.initial_master_nodes` 默认会扫描本地9300-9305端口。



### 角色

**Master：**

Master 节点负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。

Master 节点会记录一个 ClusterState，用于记录集群状态（如节点连接地址、索引等），每发生一次变化，ClusterState 的 version 就会递增。Master 节点会将 ClusterState 同步给集群中其他的节点（分为两个步骤，第一步请求更新，收到节点的响应后就会正式发起更新，节点收到更新后会再次发送确认收到更新）。



**Data：**

Data 节点可以存储、处理数据。



**Coordinating：**

协调节点，用户请求发送到协调节点，协调节点会将请求直接转发到存储我们所需文档的节点，从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 



每一个节点初始化时都拥有上述三个角色（Master 角色只有被选举上，才会成为 Master 节点）。

> 角色详情可以参考：
>
> https://www.elastic.co/guide/en/elasticsearch/reference/7.14/modules-node.html#coordinating-only-node



### Master 选举

集群一开始没有 Master 节点，因此需要选举 Master 节点。

Master 候选列表：

1. 如果是第一次的选举的，会将 `cluster.initial_master_nodes` 配置的加入到 Master 候选列表；
2. 通过节点发现阶段，在所有连接的节点中，将有资格成为 Master 节点的加入到候选列表；



* 如果节点不在候选列表中，则会不断重复发现阶段，去寻找 Master 节点。
* 如果节点在候选列表中，则会重复发现阶段，直到发现主节点或有足够的多的非主节点进行选举。

上述每次发现过程间隔为 1s，由 `discovery.find_peers_interval` 参数控制。



#### 初始化选举

 `cluster.initial_master_nodes` 配置了 Master 候选节点列表，每次选举必须要候选列表中有 `列表节点数/2 + 1` 个节点启动并加入集群才能开始。

```yaml
# 集群master候选节点
cluster.initial_master_nodes: ["node-0","node-1","node-2"]
```

当某节点投票数超过集群节点数一半时，此时该节点会成为 Master 节点。若投票数未超过一半，则继续重新选举，直到选举出 Master。

**特殊情况：**

前提：已存在一个集群 `cluster-1`，此时新增节点 `node-1` ，节点 `node-1` 配置 `cluster.initial_master_nodes: ["node-1"]` 。

此时 `node-1` 实际会选举自己作为 Master 节点并生成一个新的集群 `node`，而指定的 `cluster.name` 已经存在集群，此时会新生成一个新的 UUID 的集群（集群名相同，都叫 `cluster-1`），因此此时这两个看似集群名相同，但实际是两个不同的集群。

解决方案：

1. 删除新增节点 `node-1` 的data目录。
2. 修改 `cluster.initial_master_nodes: []`，新增节点加入集群应该设置为空或包含已有集群中的其中一个节点，避免选举出新的集群。
3. 从新启动节点，节点会加入已有集群。



#### Master 重新选举

当某个节点加入或离开集群时，主节点必须更新集群状态到各个节点，以调整同步各节点的投票配置。

如果 Master 掉线后（网络不可连或节点宕机），此时集群会重新选举新的 Master 节点。

当候选列表中某个节点发现满足以下条件时发起选举：

1. 该节点的当前状态不是 master。
2. 该节点通过 ZenDiscovery 模块的 ping 操作询问其已知的集群其他节点，没有任何节点连接到master。
3. 包括本节点在内，当前已有超过 minimum_master_nodes 个节点没有连接到master。

总结一句话：即当候选列表某个节点发现包括自己在内的多数派的master-eligible候选节点认为集群没有master时，就可以发起master选举。



候选节点优先会投票 clusterStateVersion 更大的节点（数据最新的），如果 clusterStateVersion 相同，则选择节点 id 较小。当某个节点获取当前节点数半数以上的投票时，即可成为 Master 节点。



选举时必须保证投票列表中至少存在两个存活的节点，否则不能进行选举。

投票的节点列表理论上时和候选列表一致，默认投票的节点列表会动态调整（发生节点加入和离开）。因此投票过程中只需要保证有一个节点存活即可（因为候选列表最少要有两个）。



假设有三个节点 A、B、C，节点 A发起选举，此时会有以下情况：

* 节点A 会向节点B 发送join请求，那么此时：

1. 如果节点B 已经成为Master，节点B 就会把节点A 加入到集群中，然后发布最新的 cluster_state，最新的cluster_state 就会包含节点A 的信息。相当于一次正常情况的新节点加入。对于节点A，等新的 cluster_state发布到节点A 的时候，节点A 也就完成join了。

2. 如果节点B 在竞选 Master，那么节点B 会把这次 join 当作一张选票。对于这种情况，节点A 会等待一段时间，看节点B 是否能成为真正的 Master，直到超时或者有别的 Master选成功。

3. 如果节点B 认为自己不是Master(现在不是，将来也选不上)，那么节点B 会拒绝这次join。对于这种情况，N节点A 会开启下一轮选举。

- 假设节点A 选自己当 Master，此时：

  节点A 会等别的节点来 join，即等待别的节点的选票，当收集到超过半数的选票时，认为自己成为master，然后变更 cluster_state 中的 master node为自己，并向集群发布这一消息。



上述的选举有序避免了脑裂问题：某个节点因某些原因导致无法与集群网络通信，此时该节点不会独立自己选举成主节点，从而不会出现同时出现多个相同的名字的集群。



> 为了避免投票出现偶数的情况，如果投票列表为偶数个节点时，此时会将其中一个踢出，以确保投票列表个数为奇数。



由于投票需要当前投票列表过半的约束，因此如果一次性停掉过半的节点，那么此时节点之间投票配置还没来得及动态调整，导致其投票列表节点数还是旧的，导致永远无法开始新的选举。此时只能重新启动停掉的集群，已确保投票节点过半存活。



## 分片

分片实际上可以理解为将数据按照指定的分片数进行分割。

ES 在建立索引的时候可以设置索引的主副分片数。（一旦设置将不可修改）



**主分片：**文档的变更操作（写）都在主分片上操作，节点会将主分片中的文档同步给副分片。主分片可以设置多个，主分片会尽可能均匀的分配到各个节点上（通过 Hash 后的值取模主分片数得到文档所在的主分片）。

**副分片**：作为主分片数据的备份。每个主分片可以有多个副分片，同一主分片的副本分片会落在非主分片节点，且每个节点只会有一个该副本分片。



实际上可以理解为：

1. 主分片是将数据进行分割，从而让数据均摊到多个主分片，减少写操作都落在同一个主分片上，提高负载能力和处理效率。（水平扩容）
2. 副分片是对单个主分片备份，当主分片所在节点宕掉时，此时会选取其中一个版本最高的分片作为新的主分片，从而提高数据高可用。并且副本分片还能支持执行读操作，从而提高文档读取吞吐量。（读写分离）



### 集群状态

`status` 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：

- `green`：所有的主分片和副本分片都正常分配。
- `yellow`：所有的主分片都正常分配，但不是所有的副本分片都正常分配。
- `red`：有主分片没能正常分配。



> green、yellow状态：集群能够正常运行，只是在yellow状态下可能存在数据丢失的危险（可能存在某个主分片没有副本分片备份）。
>
> yellow、red状态：出现原因有可能是因为内存空间不足或者节点挂掉。



### 索引重新分片

由于索引主分片数一旦设置了，就无法进行修改扩容。

因此如果需要修改分片数，此时只能新增一个新的索引，将旧索引的数据迁移到新的索引。



# 倒排索引原理

1. ES 会为一个分片（Shard）创建一个Lucene Index，Lucene Index 由多个 Segment（倒排索引）组成。Lucene Index 中的 Segment 由 Commit Point 负责管理。
2. 每当有文档进行新增或更新操作时，创建的索引不会直接修改磁盘中的原 Segment,而是:
   1. 写入到 translog 日志文件中，并同时也写入到index buffer缓存中。
   2. 默认 `定时1s` 或者 `缓存大小超过jvm heap 的 10%` （也可以手动调用 api）将 index buffer数据刷新到文件系统中的缓存中（此时数据可以被搜索），即创建一个新的 Segment。当 index buffer 的数据被刷入到文件系统缓存时，此时会清空 buffer 内容。
   3. translog 每隔 5s 就会写入到磁盘中。当每隔30分钟或512MB时会触发一次合并和 Commit 事件（写入一个 Commit Point，用于标识 Segment File），此时会将文件系统缓存数据 fsync 到磁盘，并将 translog 日志文件删除，重新创建一个新的 translog 文件。（该过程称为 Flush，也可以手动调用 api 触发）

3. 当删除文档时，实际上会将文档在删除文件（.del）中标识为已删除状态。（注意更新操作实际是先删除旧文档，再新增新文档）
4. 当查询文档时：
   1. 协调节点会转发到对应的分片的主分片或副分片。
   2. 分片内会根据 Commit Point 找到对应的 Segment，然后将 Segment 返回的结果合并（过滤删除列表中已删除的文档）。
   3. 协调节点将各个分片返回的数据进行合并、排序、分页等操作，然后返回给客户端。




> 第二点实际上带来了以下好处：
>
> 1. 文档新增或更新不会影响原倒排索引（不变性），因此：
>    * 写文件时不需要考虑并发问题；
>    * 倒排索引只需要被加载到内存一次后就不需要重新加载（意味着需要大量的内存空间支持）；
>    * 倒排索引从内存中读取，读取性能高；
>    * 利于压缩存储，节省磁盘空间和内存空间。
> 2. 写入 translog 日志文件是为了避免了文件系统缓存refresh过程中节点宕机，导致数据丢失问题。节点从重启时，会读取日志文件进行重新写入到 index buffer。
> 3. 避免为每一次新增和更新操作都生成一个新的 Segment 和 Segment 写入磁盘的频繁（磁盘写入慢），而是通过先写入文件系统缓存中，从而快速提供查询（提高实时性），并定时刷新写入到磁盘（减少Segment 生成数量）。
>
> 
>
> * 每个 index 对应有自己的 translog。
>
> * 为了避免 segment 过多或减少 segment 内已删除的文档，每隔30分钟会进行一次合并。多个 Segment 文件合并成一个，并将 .del 文件删除。最后会重新写入一个 Commit File 标识所有新的 Segment 文件。
>
>   

![Elasticsearch分布式架构原理(二)_elasticsearch](https://s2.51cto.com/images/blog/202009/23/e8499ae3dc831d3faae6bec7e7b051e1.jpg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=)



![image-20220830112922174](https://raw.githubusercontent.com/Cavielee/notePics/main/ELK描述.png)

