# 爬虫数据分析

## 案例

1. 爬取知乎用户个人信息，如性别、名字、地域、学校等。

2. 对爬取数据进行分析。



## 思路

1. 用户信息来源？

   知乎个人主页 url 实际由 `https://www.zhihu.com/people/` + `url_token`

 如 `https://www.zhihu.com/people/user1` 中，`user1` 即为 `url_token`，每个用户都有其唯一的 `url_token`。

​		因此只要有足够多的 `url_token` 即可获取足够多的用户个人信息。

2. `url_token` 从那获取？

   `url_token` 可以通过在用户的关注页 `https://www.zhihu.com/people/user1/following` 和被关注页 `https://www.zhihu.com/people/user1/followers` 这两个页面中进行爬虫获取。

   因此我们可以默认提供几个大V账号的 `url_token` ，然后递归爬取这些用户的关注/被关注的用户 `url_token`。

3. 如何防止反爬虫？

   由于一般网站都会记录访问的用户 IP及访问的浏览器（UA），如果某个用户频繁的访问网站，则可能会盘断该用户为爬虫用户，然后对该 IP 进行封禁。（反爬虫）

​		为了避免反爬虫识别，因此需要准备多个 UA或多个IP（代理）。

4. 免费代理逻辑：

   由于免费代理可能存在不可用的情况，因此需要进行以下处理：

   * 定时任务1：爬取免费代理存储到MQ，MQ建立消费组1进行处理（判断代理是否可用，将代理即可用情况放到DB中）
   * 定时任务2：将DB中的代理定时存放到MQ，MQ建立消费组2进行处理（判断代理是否可以，如果超过三次测试代理不可用，则从DB中删除）
   * 定时任务3：定时删除 Redis 中存放的所有代理信息，然后将 DB 中前 N 个检测可用成功次数最多的代理存储到 Redis，确保 Redis 中的代理都是高可用的。



## 数据处理

1. 数据去重。

   由于递归爬取关注和被关注的页的 `url_token`，因此存在 `url_token` 重复的情况，需要对数据进行去重。

   但由于数据量可能十分庞大，不能直接使用 Map/Set 的内存数据结构进行去重。

   可以使用布隆过滤器进行筛选。

   > 布隆过滤器逻辑：
   >
   > 1. 创建足够大的字节bitArray
   > 2. 对值进行 Hash 取余后，判断 bitArray 对应的下标值是否已置为1，如果是则该数据可能存在，如果不是则该数据不存在。
   >
   > 布隆过滤器存在一定的误差，可以通过多个 Hash 值减少误差范围。

2. 数据分析。

   可以通过将爬取到的数据存储到 ElasticSearch，并通过 Kibana 进行数据分析成可视化视图。