# 分布式一致性

分布式一致性实际指分布式架构中，系统处于数据不一致的状态，其导致的原因如下：

1. 分布式架构中，一个业务可能远程调用底层多个服务，由于网络不可靠的原因，因此存在远程调用的某个服务失败或丢失，而其他服务调用成功，从而导致数据不一致。
2. 分布式集群节点同步失败，导致集群节点之间数据不一致。



# 一致性理论

## CAP 理论

* C (一致性) Consistency：数据在多个节点上一直保持最新，即数据强一致性。
* A (可用性) Availability：非故障的节点确保一直可以在合理的时间内返回合理的响应（不是错误和超时的响应），即确保每次请求节点能返回合理的数据（可能不是最新的数据）。
* P (分区容错性) Partition tolerance：集群节点分布在多个网络分区，使得某个节点的网络故障了，整个系统任然可以继续工作。

CAP 三者是不能共有的，只能同时满足其中两点：

| 组合 | 分析                                                         |
| ---- | ------------------------------------------------------------ |
| CA   | 不满足分区容错性，因此整个系统部署在同一个网络中（如单应用系统），一旦网络故障，整个服务不可用。 |
| CP   | 不满足可用性，因此在数据同步的过程中（P），对应的服务都不能对外提供服务，从而确保数据强一致性（A）。 |
| AP   | 不满足一致性，由于分区容错且要保证可用性，因此要求同步过程中，对应的服务依旧可以对外提供服务，导致请求读取到的数据不一定是最新的。 |



## BASE 理论

BASE 理论是基于 CAP 理论中的 AP 组合。

* 基本可用(Basically Available)：分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。
* 软状态(Soft state)：允许系统中存在中间状态，这个状态不影响整体系统可用性，这里指的是 CAP 中的不一致。
* 最终一致(Eventually consistent)：最终一致是指经过一段时间后，所有节点数据都将会达到一致。



# 一致性协议

## 2PC

二阶段提交：Two-phase Commit 。

两阶段提交是一种使分布式系统中所有节点在进行事务提交时保持一致性而设计的一种协议。

两阶段提交分为两个阶段：协调者根据参与者的投票情况**（投票阶段）**进行决策处理**（提交阶段）**。

**投票阶段：**协调者（Coordinator）会向事务的参与者（Cohort）询问是否可以执行操作的请求，并等待其他参与者的响应，参与者会执行相对应的事务操作并**记录重做和回滚日志**，所有执行成功的参与者会向协调者发送 `AGREEMENT` 或者 `ABORT` 表示执行操作的结果。

**提交阶段：**当所有的参与者都返回了确定的结果（同意或者终止）时，两阶段提交就进入了提交阶段，协调者会根据投票阶段的返回情况向所有的参与者发送提交或者回滚的指令。

当事务的所有参与者都决定提交事务时，协调者会向参与者发送 `COMMIT` 请求，参与者在完成操作并释放资源之后向协调者返回完成消息，协调者在收到所有参与者的完成消息时会结束整个事务；与之相反，当有参与者决定 `ABORT` 当前事务时，协调者会向事务的参与者发送回滚请求，参与者会根据之前执行操作时的 **回滚日志** 对操作进行回滚并向协调者发送完成的消息，在提交阶段，无论当前事务被提交还是回滚，所有的资源都会被释放并且事务也一定会结束。



**存在的问题：**

- 阻塞式事务。当某个参与者发生故障或执行时间很长，其他参与者必须等待该参与者返回响应给协调者，协调者才能发送执行提交/失败命令给参与者。
  - 解决方案：可以给事务设置一个超时时间，如果某个参与者一直不响应，那么认为事务执行失败。
- 数据不一致。协调者收到参与者1的失败通知，还没来得急通知其他参与者进行回滚就发生故障，此时重启后没有了参与者1的信息而根据其他参与者成功通知进行了提交事务。
  - 解决方案：将操作日志同步到备用协调者，让备用协调者接替后续工作。



## 3PC

三阶段提交：Three-phase Commit 。是二阶段提交的优良版协议。

为了解决两阶段提交在协议的一些问题，三阶段提交引入了**超时机制** 和 **准备阶段**。

1. 准备阶段：协调者询问参与者是否可以执行事务。
2. 执行事务：若所有参与者表示可以执行事务，则会发送 ACK 让参与者执行事务。
3. 提交阶段：根据参与者事务执行结果来判断是否提交事务/回滚事务。



3PC 好处：

* 加入了超时机制，若协调者或参与者在规定时间内没有接收到来自其他节点的响应，就会根据当前状态选择提交或回滚事务。
* 当参与者发送了 ACK 之后长时间没收到协调者的响应，则会默认提交事务（因为有了第一步准备阶段，当进入第二阶段时，就证明其他节点表明都可以执行事务。因此即使协调者宕机没响应，也不会影响参与者提交事务。）



# 一致性算法

## 分类

- 强一致性

- - 说明：保证系统改变提交以后立即改变集群的状态。

  - 模型：

  - - Paxos
    - Raft（muti-paxos）
    - ZAB（muti-paxos）

- 弱一致性

- - 说明：也叫最终一致性，系统不保证改变提交以后立即改变集群的状态，但是随着时间的推移最终状态是一致的。

  - 模型：

  - - DNS系统
    - Gossip协议



## 实现举例

- Google 的 Chubby 分布式锁服务，采用了 Paxos 算法
- etcd 分布式键值数据库，采用了 Raft 算法
- Zookeeper 分布式应用协调服务，Chubby 的开源实现，采用 ZAB 算法



## 拜占庭将军问题

拜占庭将军问题指的是拜占庭有许多位将军，对于这个国家攻打哪一个国家的问题，需要多个将军去决定，每个将军都可以决定攻打或者撤退。

**问题一：**

提案发起：某个将军发起是否攻击某个国家的提案，并将该提案传达到其他将军，提案会携带该将军是否攻击的投票选择。

投票阶段：每个将军收到提案后会进行投票，投票选择攻击或者撤退。

由于将军中存在叛徒的可能，因此会导致有的将军选择了攻击，有的将军选择了撤退，即**总体投票选择不一致的问题**。

对于问题一，解决方案是每个将军将自己的投票选择发送给其他将军，使得每个将军都持有所有将军的投票选择，并对这些投票选择使用相同的规则（一般为过半投票）选取出统一的投票选择结果。这样子每个将军选择出来的结果理论上是一致的。



**问题二：**

对于问题一的解决方案存在以下问题：

假使将军中存在叛徒，该叛徒对某些将军发送的投票选择为攻击，对其他将军发送的投票结果为撤退，此时就会导致每个将军最终得到的结果可能不一致。

对于问题二，解决方案如下：

为了防止存在叛徒发送个其他将军投票选择不一的情况，提出主将军的概念。主将军是从所有将军中选取出来的，每个将军会将自己的投票选择发送给主将军，主将军收到所有将军的投票选择后，通过指定的规则（一般为过半投票）选出投票选择结果，并将该结果发送给其他将军。

> 可能会疑问如果主将军是叛徒呢？
>
> 实际上主将军的选择可以看做一个拜占庭将军问题，其应该由上一级主将军去决定该问题的最终投票选择。
>
> 即使主将军是叛徒，其做出的决定对于其他将军来说是一致的。



### 总结

拜占庭将军问题实际是指对于一个提案，如何使得各个节点得到相同的结果。

解决方案是：从所有节点中选出一个主节点（Leader），由主节点收集其他节点对该提案的投票结果，并从中作出最终选择，并将最终选择发送给其他节点。



## Paxos 算法

Paxos 算法是保证数据一致性的算法，其特点如下：

- 节点可以以任意速度运行，可能宕机、重启。但是，算法执行过程中需要记录的一些变量，在重启后应该能够恢复。
- 消息可以延迟任意长时间，可以重复(duplicated)，可以丢失(lost)，但不能损坏(corrupted)。



### 提案

提案：集群中某个节点对某个问题发起的一个提议，如节点认为某个数据值为v。

提案包含两个部分：**提案编号** + **数据值**。

**提案编号**：用于区分提案的新旧，因此提案编号应该为自增、唯一的。

**数据值**：节点对应该提案的数据的认定值。



### 角色

Paxos 算法对于节点分为三种角色：

* 提案者（Proposer）：负责发起提案。
* 接受者（Acceptor）：接收提案者的提案，并判断是否接受提案。
* 学习者（Learner）：不需要判断，直接接受收到的提案。



### Paxos 算法流程

Paxos 算法分为两个阶段：

**阶段一：准备阶段**

1. Proposer 发起提案（提案编号n）并发送给所有 Acceptor（实际上只需要半数以上即可）。
2. Acceptor 收到提案后：
   * 如果之前没有接受过提案，则承诺不再接受编号小于n的提案，并响应。
   * 如果已经接受过提案，则响应已经接受过的编号小于N的最大编号的提案。

> 半数以上是保证 Proposer 得到的结果是从绝大多数的 Acceptor 中总结的，而不是片面结果。

**阶段二：接受阶段**

1. 如果 Proposer 收到了半数以上的Acceptor的响应，那么它就可以生成编号为n，Value为v的提案[n,v]。

   * 如果所有的响应都没有提案，则表明这是一个全新的提案，此时提案v可以由 Proposer 自定义。
   * 如果响应中有提案，则表明已经有提案被接受，并且这个数据值 v 已经被选定了，为了避免新的提案（本提案）的 v 和已经被选定的 v 不一致，导致数据在多个节点存在不一样的值，因此会从所有的响应中选择编号最大的提案的Value作为本提案（当前最新的提案）的 v。

   > 上述数据不一致情况举例：假如有5个节点，节点1-4 通过提案1选定值为1。而此时新的提案[2,2]提出并发送给节点 2-5，最后提案被接受，则会造成节点1的数据值为1，节点2-5数据值为2的数据不一致情况。因此我们说新的提案值v应该为被接受的最大编号的提案v。

2. 生成提案后，Proposer 将该提案发送给半数以上的Acceptor集合，并期望这些 Acceptor 能接受该提案。（注意：此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合）

3. Acceptor 收到一个编号为n的提案的 Accept 请求，会有两种情况：

   1. 该 Acceptor 没有对编号大于n的 Prepare 请求做出过响应，它就接受该提案返回响应给 Proposer，并且将该提案发送给 Learner（Learner 无条件接受提案）。
   2. 该 Acceptor 已对编号大于n的 Prepare 请求做出过响应，因此对于该编号n的提案选择忽略。



**总结：**

1. 第一阶段实际为发起 Prepare 请求，通过 Prepare 请求完成以下两点：
   * 约束 Acceptor 可以接受的提案编号，防止 Acceptor 阶段收到旧的提案（旧的编号）。
   * 约束 Proposer 提案的数据值，如果已经有提案被接受了，应该使用已被选定的值，防止使用新值导致数据不一致。
2. 第二阶段实际为通知 Acceptor 接受提案值（前提是该提案是最新的），Acceptor 接受提案后会将该提案转发给 Learner。



### 活锁问题

假定有两个 Proposer 同时发起提案：

1. Proposer 1 执行阶段一，发起提案[1,v] 准备请求；
2. Proposer 2 执行阶段一，发起提案[2,v] 准备请求；
3. Proposer 1 执行阶段二，发起提案[1,v] 接受请求，由于第二步导致提案[1,v]视为旧提案，因此被忽略。此时Proposer 1重新发起新的提案[3,v] 准备请求；
4. Proposer 2 执行阶段二，发起提案[2,v] 接受请求，由于第三步导致提案[2,v]视为旧提案，因此被忽略。此时Proposer 2重新发起新的提案[4,v] 准备请求；

上述这种情况依次进行，则会导致死循环，永远没有一个 Proposer 执行完阶段二，导致没有提案被接受，即一种活性死锁。

为了防止这种极端情况出现，一般通过决策选定一个 Proposer 作为主Proposer，所有的提案都有该主 Proposer 发起，从而避免活锁问题。



## Raft 算法

Raft 算法是 Paxos 算法的一种简化实现。

> Redis 中哨兵集群中的哨兵 Leader 选举和 Redis-Cluster 中的 Master 节点选举都是基于Raft算法实现的。

http://thesecretlivesofdata.com/raft/

### 角色

Raft 算法中有三种角色：

* Leader 领导者：负责系统所有修改操作，并将这些结果（数据）同步到各个 Follower 节点（一般通过日志）。
* Candidate 候选者：Leader 的候选人。如果系统没有 Leader，则会从 Candidate 中选出一个当选 Leader。
* Follower 追随者：所有节点一开始都是 Follower，如果没有收到 Leader 消息，则会变成 Candidate 参与 Leader 竞选；当有 Leader 竞选出来后，Follower 则负责接收 Leader 发送的日志，进行数据同步。



### Raft 算法流程

Raft 算法流程实际上分为两种：

#### Leader 选举

Leader 角色实际上是拜占庭将军问题的主将军解决方案实现，通过 Leader 作为系统的决策者，从而避免多个决策者导致数据不一致的问题。

Leader 选举分为两种情况：

**情况一：初始选举**

1. 集群中所有节点一开始都是 Follower 角色。
2. 每个节点都会随机生成一个 timeout 时间（150ms~300ms），可以看做是一个定时器。当节点一定时间内没有收到 Leader 发送的心跳包，并且 timeout 时间到期，则此时节点会成为 Candidate，并向其他节点发起 Leader 选举请求，该请求中会附带票据，票据内容为节点选择自己作为 Leader 节点。
3. 其他节点收到选举请求后，会将收到的票据内容作为自己的票据内容进行投票，并返回响应给发起者。
4. 如果发起者收到半数以上的投票，则该节点为新的 Leader 节点。



> 选举出来的 Leader 节点会对其他 Follower 节点发送心跳包，Follower 节点收到心跳包后会将 timeout 时间清零，重新倒计时，从而避免 Leader 存在情况下，重新发起 Leader 选举节点的问题。因此需要确保 timeout 时间一定比心跳包间隔长。



上述过程中存在两个问题：

**周期（Term）问题：**

由于网络原因（如丢失、延迟、重试等），节点可能收到多个选举请求并作出投票。那么此时发起选举的节点可能收到来自同一个节点的多个投票。而这多个投票实际只有发起选举者最新一次发起对应的投票才有效，而其他票应该标识为无用的票。

对于该问题的解决方案：

1. 每个节点会持有一个标识：周期（Term）。
2. 当节点发起选举请求时，会对递增周期，并在选举请求中自己的投票附上递增后周期。
3. 节点收到选举请求时，会有两种情况：
   * 如果请求中的票据所在的周期比自身记录周期小，则表示该票据是来自旧的选举请求，即无用请求，应该丢弃。
   * 如果请求中的票据所在的周期比自身记录周期大，则更新自身记录的周期为票据的周期，并将票据的内容作为自己的票据内容进行投票响应回请求发起者。



**多 Candidate 问题：**

假设刚好有两个 Follower 的 timeout 到期，则这两个节点会同时变成 Candidate 并发起选举请求（各自都投自己为 Leader）。如果集群节点数为偶数，则此时可能存在结果为两个 Candidate 各获得一半的投票数，因此需要重新发起新的一轮投票。

为了避免无限循环结果：两个 Candidate 各获得一半的投票数，又重新发起新的一轮投票。

因此对于 timeout 时间应该设置为随机数，从而避免同一时间多个节点 timeout 时间相同，同时发起选举请求。

> 虽然 timeout 时间设置随机数，但还是存在刚好相同的可能性，因此建议集群节点为奇数，则确保至少有一个 Candidate 获取到的投票数过半。



**情况二：Leader 故障重新选举**

由于 Leader 可能存在宕机或网络故障导致不可用，因此需要一种机制重新选举出新的 Leader 节点。

1. 如果某个 Follower 节点 timeout 到期了（由于没有收到 Leader 发来心跳包），此时判定 Leader 不可用，重新发起选举请求，逻辑和上述一致。
2. 挂掉的 Leader 重启后，收到新的 Leader 发起的心跳包，发现周期不一致，则表明已经进行了新的一轮选举，此时会更新周期，并将自己转成 Follower 节点。



#### 复制日志

复制日志实际是数据同步的一个过程：

1. Leader 节点会接受客户端的事务（修改）操作，并将数据结果记录在本地日志（数据处于 uncommitted 状态）。
2. Leader 节点会在下一次发送心跳包的时候附带上提案（提案内容为该数据）。
3. Follower 节点收到心跳包中的提案后，会将数据记录在本地日志中（数据处于 uncommitted 状态），然后发送响应给 Leader。
4. Leader 节点如果收到过半的节点的响应（包括自身），则会将本地日志的该数据修改为 commit 状态（表示数据已提交），然后返回响应给客户端。
5. Leader 节点会发送 AppendEntries 请求给 Follower 节点。
6. Follower 节点收到 AppendEntries 请求后，会将本地日志对应的数据修改为 commit 状态。



**网络分割问题**

由于网络异常，可能会导致集群中出现网络分割，即集群中的部分节点之间不能互相通信。

如集群中5个节点本来可以互相通信，但由于网络异常，导致节点1-3可以互相通信，节点4-5可以互相通信，从而出现网络分割成两个集群。

<img src="C:\Users\63190\Desktop\pics\Raft网络分割1.jpg" alt="Raft网络分割1" style="zoom:67%;" />

由于网络分割导致两个新的小集群，此时每个小集群会各自重新选出一个新的 Leader。

<img src="C:\Users\63190\Desktop\pics\Raft网络分割2.jpg" alt="Raft网络分割2" style="zoom:67%;" />

此时两个小集群会存在一个多数派集群（节点数量为原先的半数以上），另一个则为少数派集群（节点数量为原先的半数以下）。

此时少数派集群的 Leader 无论发起什么提案，数据都只会被记录在本地日志（处于 uncommitted 状态），而不会被 commit（应为 Leader 节点收不到过半的响应），而多数派集群则会正常复制日志。

当网络恢复时，所有节点可以互相通信，此时集群处于数据不一致状态（原少数派集群的节点会有 uncommitted 数据，原多数派集群会有 commit 数据）。Raft 算法会进行数据恢复：

1. 少数派集群的 Leader 会退回 Follower 状态，并告诉客户端请求操作失败（因为 uncommitted 数据不会被 commit）。
2. 原多数派集群的 Leader 依旧为 Leader，原少数派集群的节点收到 AppendEntries 请求后，会对 uncommitted 数据进行删除，并同步原多数派集群中新增的 commit 数据。



## ZAB 协议

ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。

ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持 **崩溃恢复** 的 **原子广播** 协议。

基于该协议，Zookeeper 实现了一种 **主备模式** 的系统架构来保持集群中各个副本之间数据一致性。



### 角色

ZAB 中将节点分为三种角色：

* Leader 领导者：负责接收处理客户端的所有操作，如果是事务（修改）操作则会将对应结果（数据）封装成事务提议（Proposal）原子广播给所有 Follower 节点，从而实现数据同步，即数据一致性。
* Follower 追随者：负责接收处理客户端的读操作（如果是事务请求会转发给 Leader 执行）。对于 Leader 发送的事务提议（Proposal）进行同步，并参与 Leader 选举投票。
* Observer 观察者：和 Follower 唯一不同的是不参与 Leader 选举投票。



### 名词解释

* myid：服务器ID，每个服务器都有一个唯一的id。

* zxid：事务id 也叫当前服务器中最新的数据ID。Zxid 是一个 64位的数字，其高 32 位是 epoch（如果有新的 Leader ，那么该 epoch 会自增），低32位用来递增计数（每增加一个事务提议 Proposal，即数据发生变更同步，则会递增生成一个新的 Zxid）。

  每当选举产生一个新的 Leader ，就会从这个 Leader 服务器上取出本地事务日志充最大编号 Proposal 的 zxid，并从 zxid 中解析得到对应的 epoch 编号，然后再对其加1，并将低32位数字归零，从而生成一个全新的 zxid。

* 投票 epoch：选举投票轮次。同一轮投票过程中的 epoch 值是相同的，每投完一次票这个数据就会增加。用于与接收到的投票信息中的 epoch 比较，根据不同的值做出不同的判断。

> Zxid 中的 epoch 和投票 epoch 不一样：
>
> * 前者是用于区分是否发生了 Leader 变更，如果节点收到的 Proposal 中的 Zxid 是旧的 epoch（判断高 32 位），则会丢弃消息。
> * 后者是用于选举过程中，标记当前投票处于那一轮，如果是旧的 epoch 轮次投票，则应该忽略掉。

### 流程

#### 初始选举

每个节点启动时节点状态都为 looking（竞选状态）。如果集群只有一个节点，则不会开始选举，当集群多于一个节点，就会开始进行 Leader 选举。

1. 更新 epoch，标识发起新的一轮选举。

2. 每个节点都会持有自己投票（一开始都会投票给自己），投票内容为（myid、zxid、投票 epoch），然后将自己的投票广播给集群中其他节点。

3. looking 状态的节点会不断接收其他节点发送的投票，并对接收到的投票进行一下判断：

   1. 对于 looking 状态的投票：

      1. 如果投票的 epoch 比自身的 epoch 大。说明自身节点的投票轮次是旧的，因此需要更新自身的 epoch，同时清空 recvset。然后将该投票跟自己持有的投票进行比较，看是否需要更新自己的投票，比较规则如下：

         1. 比较 zxid。 zxid 越大越优先，表示节点数据越新；
         2. 其次比较 myid，myid 大者优先。

         如果收到的投票优于自身持有的投票，则会将收到的投票作为自身投票，然后广播给其他节点。

      2. 如果投票的 epoch 比自身的 epoch 小。说明对方节点处于旧的 epoch，忽略该投票。

      3. 如果投票的 epoch 等于自身的 epoch。说明两个节点处于同一轮投票中，和上述一样将该投票和自身投票进行比较，如果收到的投票优于自身持有的投票，则会将收到的投票作为自身投票，然后广播给其他节点。
      
      4. 对于 epoch 大于或者等于的情况，都意味着收到的投票是有意义的，因此会将其存放在 receset 集合中，用于后续统计票数使用。

   > 如果开始竞选后，有新的节点加入集群，一般情况下该新节点处于旧的 epoch，其发起的投票就会触发上面的第二种情况（投票的 epoch 比自身的 epoch 小），其收到的投票就会触发第一种情况（投票的 epoch 比自身的 epoch 大）。最终集群节点都会同步在一个新的 epoch 轮次中进行投票，即第三种情况（投票的 epoch 等于自身的 epoch）。

4. 对收到的投票处理完后，会判断是不是已经收集到了本轮 epoch 所有节点的投票：

   1. 如果收集完，则会根据投票结果判断是否有过半的节点投票自己作为 Leader，如果有则设置自己的角色为 Leader，反之则为 Follower，最后退出选举过程。
   2. 如果没有收集完，则可以判断已有投票中是否有节点获得了过半的支持成为 Leader，如果有则尝试在200ms 内接收投票（防止有新的节点加入，导致投票结果变更），如果没有新的投票，则表明集群中的节点认可了该投票结果，同样也设置角色并退出选举过程。

![image-20211028174328316](C:\Users\63190\AppData\Roaming\Typora\typora-user-images\image-20211028174328316.png)

#### 数据同步（原子广播）

原子广播实际上为消息广播，是一种类 2PC 协议。

Leader 负责将一个客户端的事务请求转换为一个事务提议（Proposal），并将该 Proposal 分发给集群中的所有Follower。之后等待所有Follower的反馈，一旦超过半数的 Follower 进行了正确的反馈，那么Leader会再次向所有 Follower 发起 Commit 消息，要求将上一次 Proposal 进行提交。

具体流程如下：

1. 客户端发起一个事务（修改）请求，如果是 Follower 节点接收到则会转发给 Leader 节点。

1. Leader 接收到消息请求后会将其转换成事务提议（Proposal），并为该 Proposal 生成一个全局唯一的 64位 自增 ID，叫 Zxid。（Zxid 越大意味着数据越新）
2. Leader 为每个 Follower 准备了一个 FIFO 队列（通过 TCP 协议来实现，以实现了全局有序这一个特点）将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。
3. 当 Follower 节点接收到 proposal，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。
4. 当 Leader 接收到半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，Leader 就会向所有的 Follower 节点广播 commit 消息，然后自身会在本地完成事务提交。
5. 当 Follower 收到 commit 消息以后，会将对应的 Proposal 提交。



**ZAB 协议和 2pc 区别：**

ZAB 协议和 2pc 事务不一样的地方在于，ZAB 协议不能终止事务， Follower 节点要么 ACK 给 Leader ，要么抛弃 Leader ，因此 Leader 只需要收到过半数的节点响应，即可认为可以提交 Proposal（2pc 则需要阻塞等待收到所有节点的响应才能作出提交/回滚决策）。该做法虽然会导致某一个时刻 Follower 节点和 Leader 节点的数据不一致，但最终所有节点都会处于数据一致性。以为避免了 2pc 的阻塞等待问题，因此提升了集群的整体性能。



**通信方式：**

客户端与节点之间的通信采用 NIO 的方式。

Leader 和 Follower 节点之间的通信采用 TCP 方式。Leader 为每个 Follower 维护一个消息队列，从而实现异步解耦。



#### 崩溃恢复

**崩溃导致的问题**

（一）不能处理事务请求

ZAB 协议通过 Leader 节点处理事务操作，并通过原子广播将事务消息发送到 Follower 节点，从而实现数据同步，因此 ZAB 协议正常运作基于 Leader 节点可用的情况。

当运行期间 Leader 不可用，此时没有 Leader 节点接收处理客户端的事务请求，即处于崩溃状态。

> 对于该问题，需要在 Leader 崩溃时，重新从 Follower 节点中选出新的 Leader 节点，从而保证服务可用。

（二）数据不一致

1. 当 Leader 收到过半 Follower 节点对 Proposal 的 ack 响应时，此时 Leader 节点会发送 commit 消息给所有 Follower 节点，并提高本地的 Proposal。如果在 Leader 发送 commit 消息给所有 Follower 节点的途中崩溃不可用，那么可能会导致部分 Follower 节点 commit 了该 Proposal，部分没有 commit 该 Proposal，即节点之间产生数据不一致的情况。

> 对于该问题，需要选出 Zxid 最大的 Follower 节点作为新的 Leader（确保新的 Leader 拥有最新的数据，即新的 Leader 保证 commit 了旧的 Leader 发起 commit 消息的 Proposal），新的 Leader 会将所有 commit 过的 Proposal 同步给所有的 Follower 节点。

2. 当 Leader 原子广播了 Proposal 消息后，还未收到过半 Follower 节点的 ack 响应就崩溃，导致 uncommitted 的 Proposal 可能存在 Follower 节点中。

> 对于该问题，新的 Leader 的 Zxid 高 32 位代表的 epoch 自增，从而使得 Follower 节点收到新的 Leader 的 Zxid 后，分析获得最新的 epoch，并将自身旧的（epoch 比最新的 epoch小）未提交的 Proposal 删掉。



因此 Leader 节点会和 Follower 节点进行数据同步，当超过半数的节点完成数据同步后，ZAB 协议就会退出恢复模式。

崩溃恢复具体流程如下：

**运行期间 Leader 选举**

流程如下：

1. 当 Leader 崩溃或者 Leader 失去过半的 Follower 节点联系，这时会进入崩溃恢复模式，所有非 Observer  节点服务器状态变为 **looking（竞选状态）**。

2. 更新 epoch，标识发起新的一轮选举。

3. 每个节点都会持有自己投票（一开始都会投票给自己），投票内容为（myid、zxid、投票 epoch），然后将自己的投票广播给集群中其他节点。

4. looking 状态的节点会不断接收其他节点发送的投票，并对接收到的投票进行一下判断：

   1. 对于 looking 状态的投票：

      1. 如果投票的 epoch 比自身的 epoch 大。说明自身节点的投票轮次是旧的，因此需要更新自身的 epoch，同时清空 recvset。然后将该投票跟自己持有的投票进行比较，看是否需要更新自己的投票，比较规则如下：

         1. 比较 zxid。 zxid 越大越优先，表示节点数据越新；
         2. 其次比较 myid，myid 大者优先。

         如果收到的投票优于自身持有的投票，则会将收到的投票作为自身投票，然后广播给其他节点。

      2. 如果投票的 epoch 比自身的 epoch 小。说明对方节点处于旧的 epoch，忽略该投票。

      3. 如果投票的 epoch 等于自身的 epoch。说明两个节点处于同一轮投票中，和上述一样将该投票和自身投票进行比较，如果收到的投票优于自身持有的投票，则会将收到的投票作为自身投票，然后广播给其他节点。

      4. 对于 epoch 大于或者等于的情况，都意味着收到的投票是有意义的，因此会将其存放在 receset 集合中，用于后续统计票数使用。

      > 如果开始竞选后，有新的节点加入集群，一般情况下该新节点处于旧的 epoch，其发起的投票就会触发上面的第二种情况（投票的 epoch 比自身的 epoch 小），其收到的投票就会触发第一种情况（投票的 epoch 比自身的 epoch 大）。最终集群节点都会同步在一个新的 epoch 轮次中进行投票，即第三种情况（投票的 epoch 等于自身的 epoch）。

   2. 对于 leading/following 状态的投票：

      1. 如果投票的 epoch 等于自身的 epoch，将该数据保存到 recvset。如果是 Leading 状态，则表明已经有节点获得投票结果，并且该节点票选其自身为 Leader。因此需要校验本节点收集的投票是否有半数以上的节点选举它，如果校验通过，则将自己选举状态改为 following 并退出选举过程。
      2. 如果投票的 epoch 不等于自身的 epoch，此时情况可能是集群节点已经获得投票结果（选定 Leader 节点），但本节点为新加入节点，实际上可能会影响已经获得的投票结果，因此需要将投票保存到 outofelection 中，再根据 outofelection 来判断本节点收集的投票是否有半数以上的节点选举出来的 Leader 与投票中选定的 Leader 是否一致，如果一致，则更新 epoch，并设置选举状态、退出选举过程。

5. 对收到的投票处理完后，会判断是不是已经收集到了本轮 epoch 所有节点的投票：

   1. 如果收集完，则会根据投票结果判断是否有过半的节点投票自己作为 Leader，如果有则设置自己的角色为 Leader，反之则为 Follower，最后退出选举过程。
   2. 如果没有收集完，则可以判断已有投票中是否有节点获得了过半的支持成为 Leader，如果有则尝试在200ms 内接收投票（防止有新的节点加入，导致投票结果变更），如果没有新的投票，则表明集群中的节点认可了该投票结果，同样也设置角色并退出选举过程。



 **数据恢复**

1. 完成 Leader 选举后（新的 Leader 具有最高的zxid），会将自身的本地事务日志中的所有 Proposal 和所有 Follower 节点中的所有 Proposal 进行对比。对比过程中会进行以下数据恢复：
   1. 确保 Follower 节点对 Leader 节点已经提交过的 Proposal 进行提交。
   2. 对于节点在上一个 epoch 中未被 commit 的 Proposal 进行回滚。
2. 进行完上述的数据恢复后，Leader 才会把该 Follower 加入到真正可用的 Follower 列表中，当有过半的 Follower 节点完成数据恢复（数据和 Leader 保持一致），则 Leader 可以开始工作（接收事务请求，然后提出新的 Proposal）。
3. 当新的节点加入到集群中时，节点会直接进入数据恢复模式，和 Leader 节点进行数据同步。同步完成后即可正常对外提供非事务请求的处理。



### 总结

整个 Zookeeper 就是在这两个模式之间切换。简而言之，当 Leader 服务可以正常使用，就进入消息广播模式，当 Leader 不可用时，则进入崩溃恢复模式。

基于该协议，Zookeeper 实现了一种 **主备模式** 的系统架构来保持集群中各个副本之间数据一致性。其中所有客户端写入数据都是写入到 Leader 节点中，然后，由 Leader 同步到 Follower 节点中。
