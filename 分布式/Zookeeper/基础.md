Zookeeper 是一个高可靠的分布式协调中间件。



# 数据模型

![Zookeeper节点](C:\Users\63190\Desktop\pics\Zookeeper节点.png)

客户端可以通过 Zookeeper 中间件进行数据存储。

Zookeeper 数据模型实际类似于文件系统的层次化树形结构，即文件夹（节点）里面可以有子文件夹，子文件夹也可以有其子文件夹，以此类推。

ZNode 是 Zookeeper 的最小单元。每一个 ZNode 都可以存储数据和子节点。

通过节点可以存储数据，并且根据节点和子节点的特性可以解决分布式架构中的相关问题。



由于 `zookeeper` 是目录节点结构，在获取和创建节点时，必须要以`“/”` 开头，否则在获取节点时会报错 `Path must start with / character`。因此要获取某个子节点时，要输入其完整的目录结构。

> Znode 默认可存储最大数据量为 1MB。
>
> 一个节点的数据量不仅包含它自身存储数据，它的所有子节点的名字也要折算成Byte数计入，因此 Znode 的子节点数也不是无限的。



## 节点属性

一个`Znode` 节点不仅可以存储数据，还有一些其他特别的属性。

| 节点属性       | 注解                                                         |
| :------------- | :----------------------------------------------------------- |
| cZxid          | 该数据节点被创建时的事务Id                                   |
| mZxid          | 该数据节点被修改时最新的事务Id                               |
| pZxid          | 当前节点的父级节点事务Id                                     |
| ctime          | 该数据节点创建时间                                           |
| mtime          | 该数据节点最后修改时间                                       |
| dataVersion    | 当前节点版本号（每修改一次值+1递增）                         |
| cversion       | 子节点版本号（子节点修改次数，每修改一次值+1递增）           |
| aclVersion     | 当前节点acl版本号（节点被修改acl权限次数，每修改一次值+1递增） |
| ephemeralOwner | 临时节点标识，当前节点如果是临时节点，则存储的创建者的会话id（sessionId），如果不是，那么值=0 |
| dataLength     | 当前节点所存储的数据长度                                     |
| numChildren    | 当前节点下子节点的个数                                       |



### Zxid

`Znode` 节点状态改变会导致该节点收到一个 `zxid` 格式的时间戳，这个时间戳是全局有序的，Znode节点的建立或者更新都会产生一个新的。

每个znode节点都有3个 `zxid` 属性：

* `cZxid`：节点创建时间；
* `mZxid`：节点修改时间，与子节点无关；
* `pZxid`：节点或者该节点的子节点的最后一次创建或者修改时间，孙子节点无关。

`zxid`属性主要应用于`zookeeper`的集群，这个后边介绍集群时详细说。



### Version

`Znode` 属性中一共有三个版本号:

* `dataversion`：数据版本号；
* `cversion`：子节点版本号；
* `aclversion`：节点所拥有的ACL权限版本号。

`Znode` 中的数据可以有多个版本，如果某一个节点下存有多个数据版本，那么查询这个节点数据就需要带上版本号。

每当我们对 `Znode` 节点数据修改后，该节点的 `dataversion` 版本号会递增。

当客户端请求该 `Znode` 节点时，会同时返回节点数据和版本号。

当 `dataversion` 为  `-1` 的时候可以忽略版本进行操作。

对一个节点设置权限时 `aclVersion` 版本号会递增。



## 节点类型

`zookeeper` 有四种类型的 `Znode`，在用客户端 `client` 创建节点的时候需要指定类型。



### 持久化节点

PERSISTENT：持久化目录节点 。

客户端 client 创建节点后，与 zookeeper 断开连接该节点将被持久化，当 client 再次连接后节点依旧存在。



### 持久化有序节点

PERSISTENT_SEQUENTIAL：持久化顺序节点。

客户端 client 创建节点后，与 zookeeper 断开连接该节点将被持久化，再次连接节点还存在。zookeeper 会给持久化有序目录中的节点名称进行顺序编号，例如 `/lock/`目录下创建三个节点，会得到如下节点名称：/lock/0000000001、/lock/0000000002、/lock/0000000003。



### 临时节点

EPHEMERAL：临时目录节点。

客户端 client 与 zookeeper 断开连接后，该节点即会被删除。



### 临时有序节点

EPHEMERAL_SEQUENTIAL：临时顺序节点。

客户端 client 与 zookeeper 断开连接后，该节点被删除，会给该节点名称进行顺序编号，例如：/lock/0000000001、/lock/0000000002、/lock/0000000003。



### 未完善

CONTAINER：当子节点都被删除后， Container 也随即删除

PERSISTENT_WITH_TTL：超过 TTL 未被修改，且没有子节点

PERSISTENT_SEQUENTIAL_WITH_TTL：客户端断开连接后不会自动删除 Znode ，如果该 Znode 没有子 Znode 且在给定 TTL 时间内无修改，该 Znode 将会被删除； TTL 单位是毫秒，必须大0 且小于或等于EphemeralType.MAX_TTL



## 节点的 ACL 权限控制

ACL（Access Control List）—— 节点的权限控制。

为了保证 zookeeper 中的数据的安全性，避免误操作带来的影响，通过 `ACL` 机制来解决 `Znode` 节点的访问权限问题，要注意的是 `zookeeper` 对权限的控制是基于 `znode` 级别的，也就说节点之间的权限不具有继承性，即子节点不继承父节点的权限。

`zookeeper` 中设置 ACL 权限的格式由 `<schema>:<id>:<acl>` 三段组成。

**schema** ：表示授权的方式

- `world`：表示任何人都可以访问。（默认权限）
- `auth`：只有认证的用户可以访问
- `digest`：使用 username:password 用户密码生成 MD5 哈希值作为认证 ID
- `host/ip`：使用客户端主机IP地址来进行认证

**id**：权限的作用域，用来标识身份，依赖于 schema 选择哪种方式。

**acl**：给一个节点赋予哪些权限，节点的权限有 create、delete、write、read、admin 统称 `cdwra`。



### world 授权方式

world 权限是 `znode` 节点默认权限，表示任何人都可以访问。



### auth 授权方式

schema 用 `auth` 授权表示只有认证后的用户才可以访问，那么首先就需要添加认证用户，添加完以后需要对认证的用户设置ACL权限。

auth 设置用户密码是明文。

```shell
// 添加 auth 用户，格式为：用户名/用户密码
addauth digest user:password
// 设置用户 user 的 acl 权限
setAcl /test auth:user:crdwa
```



### digest 授权方式

用户名：密码方式授权是针对单个特定用户，这种方式是不需要先添加认证用户的。

如果在代码中使用 zookeeper 客户端设置ACL，那么密码是明文的，但若是 zk.cli 等客户端操作就需要将密码进行 `sha1` 及 `base64` 处理。

可以使用 Zookeeper 提供的 DigestAuthenticationProvider 类生成加密密码。



### ip 授权方式

通过对特定的IP地址，也可以是一个 IP 段进行授权。



# 高可用

Leader选举机制



# 数据同步





# watcher 机制

`watcher` 是 `zooKeeper` 中一个非常核心功能 ，客户端 `watcher`  可以监控节点的数据变化以及它子节点的变化，一旦这些状态发生变化，zooKeeper 服务端就会通知所有在这个节点上设置过 `watcher` 的客户端 ，从而每个客户端都很快感知，它所监听的节点状态发生变化，而做出对应的逻辑处理。



## watcher 类型

watcher 类型分为两种：

* **DataWatches**：znode节点的数据变更从而触发 `watch` 事件，触发条件`getData()`、`exists()`、`setData()`、 `create()`。

* **Child Watches**：znode 的子节点发生变更触发的 watch 事件，触发条件 `getChildren()`、 `create()`。

当调用 `delete()` 方法删除 znode 时，则会同时触发 `Data Watches` 和 `Child Watches`，如果被删除的节点还有父节点，则父节点会触发一个 `Child Watches`。

## watcher 特性

`watch` 对节点的监听事件是一次性的。客户端在指定的节点设置了监听 `watch`，一旦该节点数据发生变更通知一次客户端后，客户端对该节点的监听事件就失效了。

如果还要继续监听这个节点，就需要我们在客户端的监听回调中，再次对节点的监听 `watch` 事件设置为 `True`。否则客户端只能接收到一次该节点的变更通知。



## 事件类型

|        zookeeper 事件         |                           事件含义                           |
| :---------------------------: | :----------------------------------------------------------: |
|     EventType.NodeCreated     |               当监听节点被创建时，该事件被触发               |
| EventType.NodeChildrenChanged | 当监听节点的直接子节点被创建、被删除、子节点数据发生变更时，该事件被触发。 |
|   EventType.NodeDataChanged   |           当监听节点的数据发生变更时，该事件被触发           |
|     EventType.NodeDeleted     |              当监听节点被删除时，该事件被触发。              |
|        EventType.None         | 当 zookeeper 客户端的连接状态发生变更时，即 KeeperState.Expired、KeeperState.SyncConnected、KeeperState.AuthFailed |



# Zookeeper 分布式协调

通过 Zookeeper 的特性，可以实现分布式协调，如分布式锁、服务注册中心、分布式队列、配置中心、选举等功能。



## 分布式锁

`zookeeper` 基于 `watcher` 机制和 `znode` 的有序节点可以很好的实现分布式锁。

首先创建一个 `/test/lock` 父节点表示一把锁，尽量是持久节点（PERSISTENT类型），每个尝试获取这把锁的客户端，在 `/test/lock` 父节点下创建临时顺序子节点。

逻辑如下：

1. 首先创建一个节点代表锁，如 `/user/regist/lock` （一般才用持久化节点，因为锁会经常用）
2. 客户端获取锁，实际是向锁节点添加一个临时有序节点。
3. 通过规定序号最小的节点获得锁。

由于序号的递增性，通过规定序号最小的节点即获得锁。

例如：客户端来获取锁，在 `/test/lock` 节点下创建节点为`/test/lock/seq-00000001`，它是最小的所以它优先拿到了锁，其它节点等待通知再次获取锁。`/test/lock/seq-00000001` 执行完自己的逻辑后删除节点释放锁。

**那么节点`/test/lock/seq-00000002`想要获取锁等谁的通知呢？**

这里我们让`/test/lock/seq-00000002`节点监听`/test/lock/seq-00000001`节点，一旦`/test/lock/seq-00000001`节点删除，则通知`/test/lock/seq-00000002`节点，让它再次判断自己是不是最小的节点，是则拿到锁，不是继续等通知。

以此类推`/test/lock/seq-00000003`节点监听`/test/lock/seq-00000002`节点，总是让后一个节点监听前一个节点，不用让所有节点都监听最小的节点，避免设置不必要的监听，以免造成大量无效的通知，形成“羊群效应”。

`zookeeper`分布式锁和`redis`分布式锁相比，因为大量的创建、删除节点性能上比较差，并不是很推荐。

## 服务注册中心

在分布式架构中，服务调用需要进行远程网络通信。因此需要知道各个服务集群节点的地址信息（服务不一定会有集群，可能只是单节点）。

而 Zookeeper 可以作为第三方存储中间件去管理服务地址信息，服务提供者只需将自己地址信息注册（存储）到 Zookeeper 中，而服务消费者从 Zookeeper 中获取服务提供者地址信息，并且 Zookeeper 提供事件机制，用于当服务提供者不可用时主动通知服务消费者。

![Zookeeper服务注册中心](C:\Users\63190\Desktop\pics\Zookeeper服务注册中心.jpg)

- **服务注册：** 服务提供者（`Provider`）启动时，会向 `zookeeper服务端` 注册服务信息，也就是创建一个节点，例如：用户注册服务`com.xxx.user.register`，并在节点上存储服务的相关数据（如服务提供者的 ip 地址、端口等）。
- **服务发现：** 服务消费者（`Consumer`）启动时，根据自身配置的依赖服务信息，向`zookeeper服务端`获取注册的服务信息并设置 `watch监听`，获取到注册的服务信息之后，将服务提供者的信息缓存在本地，并进行服务的调用。
- **服务通知：** 一旦服务提供者因某种原因宕机不再提供服务之后，客户端与`zookeeper`服务端断开连接，`zookeeper`服务端上服务提供者对应服务节点会被删除（例如：用户注册服务`com.xxx.user.register`），随后`zookeeper`服务端会异步向所有消费用户注册服务`com.xxx.user.register`，且设置了`watch监听`的服务消费者发出节点被删除的通知，消费者根据收到的通知拉取最新服务列表，更新本地缓存的服务列表。

## 选举

集群中一般需要区分主节点和从节点。因此需要一种选举机制从集群中选取出主节点。



# Zookeeper 客户端

zookeeper 比较常用的 Java 客户端有 zkclient、curator。



## Curator

Curator 对于 zookeeper 的抽象层次比较高，简化了zookeeper 客户端的开发量。因此一般使用 Curator。

Curator 好处：

1. 封装 zookeeper client 与 zookeeper server 之间的连接处理
2. 提供了一套 fluent 风格的操作 api
3. 提供 zookeeper 各种应用场景（共享锁、 leader 选举）的抽象封装



**建立连接：**

```java
CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()
    .connectString(CONNECTION_STR)
    .sessionTimeoutMs(5000)
    .retryPolicy(new ExponentialBackoffRetry(1000,3))
    .namespace(“curator”)
    .build();
```



### 重试策略

Curator 提供以下几种重试策略：

* ExponentialBackoffRetry：重试指定的次数，且每一次重试之间停顿的时间逐渐增加
* RetryNTimes：指定最大重试次数的重试策略；
* RetryOneTime：仅重试一次；
* RetryUntilElapsed：一直重试直到达到规定的时间。



### namespace

namespace 是指定隔离命名空间，即客户端对 Zookeeper 上数据节点的任何操作都是相对 namespace 目录进行的，这有利于实现不同的 Zookeeper 的业务之间的隔离。

如 user 服务，那么 namespace 应该为 user。



### watcher 使用

Curator 提供了三种 Watcher 来监听节点的变化：

* Path Child Cache：监视一个路径下子节点的创建 、 删除 、 更新。
* NodeCache：监视当前节点的创建、更新、删除，并将结点的数据缓存在本地。
* TreeCache（Path Child Cache 和 NodeCache 结合）：监视路径下的创建、更新、删除事件，并缓存路径下所有子节点的数据。
