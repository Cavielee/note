# 数据库性能瓶颈原因

**数据库性能瓶颈的出现。**数据库性能出现问题，大致从以下几点体现：

* 无法获取连接，可能是因为在高并发的情况下连接数不够了。
* 操作数据变慢，数据库处理数据的效率出了问题。
* 存储出现问题，比如单机存储的数据量太大了，存储的问题。

归根结底都是受到了硬件的限制，比如CPU，内存，磁盘，网络等等。但是我们优
化肯定不可能直接从扩展硬件入手，因为带来的收益和成本投入比例太比。



# 性能优化方案

1. **重启数据库**（出了问题先重启，如果重启后还出现性能问题，那就从下面点再入手）。
2. **优化SQL语句（索引）**。我们可以在程序中对SQL 语句进行优化，最终的目标是用到索引。
3. **表与存储引擎**：
   1. 数据是存放在表里面的，表又是以不同的格式存放在存储引擎中的，所以我们可以选用特定的存储引擎；
   2. 对表进行分区，对表结构进行拆分或者冗余处理，或者对表结构比如字段的定义进行优化。
4. **架构**：
   1. 集群负载均衡；
   2. 主从复制实现读写分离，让写的服务都访问master 服务器，读的请求都访
      问从服务器，slave 服务器自动与 master 主服务器同步数据；
   3. 在数据库前面加一层缓存，达到减少数据库的压力，提升访问速度的目的。
   4. 分库分表（分片），将不同的数据分布到不同的服务节点，分散数据库服务的存储压力和访问压力。

> 注意主从（replicate）和分片（shard）的区别：主从通过数据冗余实现高可用，和实现读写分离。
> 分片通过拆分数据分散存储和访问压力。

5. **配置**。数据库配置的优化，比如连接数，缓冲区大小等等，优化配置的目的都是为了更高效地利用硬件。
6. **操作系统与硬件**。

上述优化方案从上往下，成本收益比慢慢地在增加。堆硬件是一种向上扩展，而像主从、分库分表是水平扩展。



# 主从复制

**主从复制**指在MySQL 多服务器的架构中，至少要有一个主节点（master），跟主节点相对的，我们把它叫做从节点（slave）。主从复制，就是把主节点的数据复制到一个或者多个从节点。主服务器和从服务器可以在不同的IP 上，通过远程连接来同步数据，这个是异步的过程。



## 主从复制的形式

* 一主一从
* 一主多从（从服集群）
* 多主一从（主服集群）
* 双主复制（互为读写）
* 级联复制（防止从服宕掉，从服也配上自己的从服集群做备份）



## 主从复制作用

* 数据备份：把数据复制到不同的机器上，以免单台服务器发生故障时数据丢失。
* 读写分离：让主库负责写，从库负责读，从而提高读写的并发。
* 高可用HA：当节点故障时，自动转移到其他节点，提高可用性。
* 扩展：结合负载的机制，均摊所有的应用访问请求，降低单机IO。



## Mysql主从复制实现

### binlog

客户端对MySQL 数据库进行操作的时候，包括DDL 和DML 语句，服务端会在日志文件中用事件的形式记录所有的操作记录，这个文件就是 `binlog` 文件（属于逻辑日志，跟Redis 的AOF 文件类似）。

**基于binlog，我们可以实现主从复制\和数据恢复。**
Binlog 默认是不开启的，需要在服务端手动配置。注意有一定的性能损耗。

#### 开启binlog

修改`/etc/my.cnf`配置文件

```txt
log-bin=/var/lib/mysql/mysql-bin
server-id=1
```

重启MySQL 服务

```sh
service mysqld stop
service mysqld start
## 如果出错查看日志
vi /var/log/mysqld.log
cd /var/lib/mysql
```

sql查看系统变量，验证是否开启binlog：

```sql
show variables like 'log_bin%';
```



#### binlog 记录格式

* STATEMENT：记录每一条修改数据的SQL 语句（减少日志量，节约IO）。
* ROW：记录哪条数据被修改了，修改成什么样子了（5.7 以后默认）。
* MIXED：结合两种方式，一般的语句用STATEMENT，函数之类的用ROW。



sql语句查看binlog 格式：

```sql
show global variables like '%binlog_format%';
```

sql语句查看binlog 列表：

```sql
show binary logs;
```

sql语句查看binlog 内容：

```sql
show binlog events in 'mysql-bin.xxxxx';
```

用mysqlbinlog 工具，基于时间查看binlog（Linux命令）：

```sh
/usr/bin/mysqlbinlog --start-datetime='2020-08-22 13:30:00' --stop-datetime='2020-08-22 14:01:01' -d cavie/var/lib/mysql/mysql-bin.000001
```



### 主从复制配置使用

1. 主库开启binlog，设置server-id

2. 在主库创建具有复制权限的用户，允许从库连接

   ```sql
   GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'repl'@'192.168.8.147' IDENTIFIED BY
   '123456';
   FLUSH PRIVILEGES;
   ```

3. 从库/etc/my.cnf 配置（从服一般配置为只读模式），重启数据库

   ```txt
   server-id=2
   log-bin=mysql-bin
   relay-log=mysql-relay-bin
   read-only=1
   log-slave-updates=1
   ```

   > log-slave-updates 决定了在从binlog 读取数据时，是否记录binlog，实现双主和级联的关键。

4. 在从库执行

   ```sql
   stop slave;
   change master to
   master_host='192.168.8.146',master_user='repl',master_password='123456',master_log_file='mysql-bin.000001',
   master_log_pos=4;
   start slave;
   ```

5. 查看同步状态

   ```sql
   SHOW SLAVE STATUS \G
   ```



### 主从复制原理

![mysql主从复制原理](https://raw.githubusercontent.com/Cavielee/notePics/main/mysql主从复制原理.jpg)

1. slave 服务器执行start slave，开启主从复制开关， slave 服务器的IO 线程请求从master 服务器读取binlog（如果该线程追赶上了主库，会进入睡眠状态）。
2. master 服务器创建Log Dump 线程，把binlog 发送给slave 服务器。slave 服务器把读取到的binlog 日志内容写入中继日志relay log（会记录位置信息，以便下次继续读取）。
3. slave 服务器的SQL 线程会实时检测relay log 中新增的日志内容，把relay log解析成SQL 语句，并执行。



### 主从复制同步延迟

对于我们 master 节点的更新操作是实时写入，而 slave 节点的更新操作则是通过其I/O线程请求 master 节点获取 binlog 后由 SQL 线程去读取中继日志（relay log）去执行一遍更新操作。因此对于 master 节点的数据是实时的，而 slave 节点的数据存在延迟。

slave 节点的数据延迟的原因：

在早期的MySQL 中，slave 节点的 SQL 线程是单线程。master 可以支持SQL 语句的并行执行，配置了多少的最大连接数就是最多同时多少个SQL 并行执行。而slave 的SQL 却只能单线程排队执行，在主库并发量很大的情况下，同步数据肯定会出现延迟。

slave 节点的 SQL 线程不能多线程并发执行是因为master 记录的 binlog （更新SQL语句）需要顺序执行，如果并发执行，可能导致执行顺序出错，从而数据错乱。



#### 主从复制同步机制

**异步复制：**

MySQL 默认主从复制是异步复制。即 master 节点写入 binlog，事务结束就会返回客户端。master 节点并不关心 slave 节点何时同步完数据。因此主从节点数据存在不一致问题（有延迟）。



**全同步复制：**

master 节点写入 binlog后，要等待所有 slave 节点同步完数据后，master 事务才结束，返回客户端。此时确保了每次读取slave 节点数据时，主从数据都是一致的。

全同步虽然解决了主从数据不一致问题，但事务执行的时间会变长，它会导致master 节点性能下降。



**半同步复制：**

半同步复制是介于异步复制和全同步复制之间的方式。

master 节点在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个 slave 节点接收到binlog 并写到relay log 中才返回给客户端。master 不会等待很长的时间，但是返回给客户端的时候，主从数据即将同步一致，因为它只剩最后一步了：slave 节点读取relay log 并写入。

MySQL 使用半同步复制，必须安装一个插件。这个插件在 mysql 的插件目录下已经有提供：

在 plugin 目录下，主库安装下面插件，并开启半同步复制开关：

```sql
-- 主库执行
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
set global rpl_semi_sync_master_enabled=1;
show variables like '%semi_sync%';
```

从库安装下面插件：

```sql
-- 从库执行
INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';
set global rpl_semi_sync_slave_enabled=1;
show global variables like '%semi%';
```



相对于异步复制，半同步复制提高了数据的安全性，同时它也造成了一定程度的延迟，它需要等待一个slave 写入中继日志，这里多了一个网络交互的过程，所以，半同步复制最好在低延时的网络中使用（可以将主从数据库放在同一个网络下）。



#### 多库并行复制

如果是多主多从时，不同的 SQL 会发送到不同的 master 节点执行，那么对于这些 master 节点的 slave 节点来说是并发复制的，因为他们数据是互不干扰的。

该方案提升了 SQL 语句的并行执行。



#### GTID 复制

在 master 节点可以并发执行多个事务，是因为这些事务之间并没有同时操作相同的数据，不存在资源竞争和数据干扰问题。因此实际上从节点对于这些事务也可以并发执行。

实际上在并发执行事务时，会将事务分组并发执行（组内事务之间不存在资源竞争和数据干扰），并给组一个编号，该编号叫做GTID（全局事务ID Global Transaction Identifiers）。slave 节点可以使用 GTID 来实现一个一个事务组的复制同步，从而使得并发执行 SQL 语句。

GTID 复制默认是关闭的，我们可以通过修改配置参数打开它：

```sql
show global variables like 'gtid_mode';
```



### 主节点高可用

高可用HA 方案需要解决的问题：

* 当一个master 节点宕机的时候，如何提升一个数据最新的slave 成为master。
* 同时运行多个master，如何解决 master 之间数据复制。
* 客户端连接路由。

不同的高可用HA 方案，实施难度不一样，运维管理的成本也不一样。



#### 主从复制

传统的HAProxy + keepalived 的方案，基于主从复制。



#### NDB Cluster

基于NDB 集群存储引擎的MySQL Cluster。

![NDB](https://raw.githubusercontent.com/Cavielee/notePics/main/NDB.jpg)



#### Galera

一种多主同步复制的集群方案。

![galera](https://raw.githubusercontent.com/Cavielee/notePics/main/galera.jpg)



#### MHA/MMM

MMM（Master-Master replication manager for MySQL），一种多主的高可用架构，像美团这样的公司早期也有大量使用MMM。MHA（MySQL Master High Available）。

MMM 和MHA 都是对外提供一个虚拟IP，并且监控主节点和从节点，当主节点发生故障的时候，需要把一个从节点提升为主节点，并且把从节点里面比主节点缺少的数据补上，把VIP 指向新的主节点。



#### MGR

MySQL Group Replicatioin（MGR），这个套件里面包括了mysql shell 和mysql-route。

![MGR](https://raw.githubusercontent.com/Cavielee/notePics/main/MGR.jpg)



# 分库分表

## 什么时候需要分库分表

分库分表并不是一开始就需要去考虑的做法，因为只有当系统架构逐渐演变，数据量十分庞大，并且数据量极速的增长时，这时候我们才需要考虑分库分表。

> 500w 的数据量作为分库分表的推荐值。



### 分库

架构大致演变过程：

1. 单应用系统单数据库。一个应用内包含了许多核心的业务系统。
2. 多应用系统单数据库。随着业务功能越来越多，系统越来越臃肿，多个业务同时访问单应用系统造成访问压力。因此将单应用系统按照业务功能拆分成多个子系统（单一部署），但这些子系统都访问同一个数据库。
3. 多应用独立数据库。由于系统越来越多，都需要访问同一个数据库，但是一个物理数据库能够支撑的并发量是有限的，所有的业务系统之间还会产生竞争，最终会导致应用的性能下降，甚至拖垮业务系统。因此将数据库根据系统划分成各个系统独立的数据库。

到了第三步就是分库的一个操作，根据业务系统将相关联的表独立成一个库。



### 分表

我们对原来一个数据库的表做了分库以后，其中一些表的数据还在以一个非常快的速度在增长，这个时候查询也已经出现了非常明显的效率下降。

所以，在分库之后，还需要进一步进行分表。当然，我们最开始想到的可能是在一个数据库里面拆分数据，分区或者分表，到后面才是切分到多个数据库中。

**分表主要是为了减少单张表的大小，解决单表数据量带来的性能问题。**



## 分库分表的类型和特点

从维度来说分成两种，一种是垂直，一种是水平。

### 垂直切分

基于表或字段划分，表结构不同。垂直切分解决的是单库数据表过多、单表字段过多的问题。

#### 单库

单库分表，比如：商户信息表，拆分成基本信息表，联系方式表，结算信息表，附件表等等。

#### 多库

多库垂直分表就是把原来存储在一个库的不同的表，拆分到不同的数据库。例如商城应用，将所有商品相关的表单独放在商品数据库中，用户相关的表单独放在用户数据库中。



### 水平切分

基于数据划分，表结构相同，数据不同。水平切分解决的是单表数据量过多的问题。

#### 单库

例如商品购买记录表，我们可以按照时间维度划分表，例如按照年月，将指定年月的记录单独放在一张表中。（不能解决单机存储瓶颈问题）

#### 多库

可以将表根据不同的切分逻辑（取余、时间等）分别存储在不同的数据库中（分布在不同数据库中的表，其表结构是一样的）。



## 分库分表带来的问题

### 跨库关联问题

例如商品购买记录需要关联用户信息，而商品购买记录在商品数据库中，用户信息在用户数据库中，此时并不能像单数据库中使用join做关联查询。

**解决方案：**

1. 字段冗余。将需要关联的用户信息字段在商品购买记录表中也存一份同样的数据。该方案避免了跨库关联查询问题，但当关联的用户信息字段发生改变，需要对这些冗余该字段的表同样做更新操作。
2. 数据同步：比如商户系统要查询产品系统的产品表，可以在商户系统创建一张产品表，通过ETL 或者其他方式定时同步产品数据。
3. 全局表（广播表）：将一些很多系统都会用到的数据，放到同一张表并在每个库中都存一份。
4. ER 表（绑定表）：例如商品购买记录他的主键是order_id，而另一张表是商品购买记录细节表，同样的该表也存储了order_id。像这样的子表存有主表的主键，我们可以称为ER表，我们只需确保两张表中order_id相同的数据落在同一个数据库中，这样子在关联查询时，就避免了跨库需求。
5. 系统层组装：在不同的数据库中分别查询所需要的数据信息，再组合一起处理返回给客户端。



### 分布式事务

例如购买商品，需要在用户系统扣钱（用户表中扣钱），在商品系统生成订单（商品购买记录表中增加记录），看似一个操作，落到数据库层面实际分成了两个操作。因此就需要确保这两个操作要不都成功，要不都失败。这就需要事务来确保，如果都在同一个数据库，就可以用本地事务控制，但现在分库后，就需要通过其他的方案解决。



#### CAP理论

* C (一致性) Consistency：对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。
* A (可用性) Availability：非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的
* P (分区容错性) Partition tolerance：当出现网络分区后，系统能够继续工作。打个比方，这里集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正工作。

CAP 三者是不能共有的，只能同时满足其中两点。基于AP，我们又有了BASE 理论。

#### BASE理论

* 基本可用(Basically Available)：分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。
* 软状态(Soft state)：允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP 中的不一致。
* 最终一致(Eventually consistent)：最终一致是指经过一段时间后，所有节点数据都将会达到一致。



**分布式事务解决方案：**

1. 全局事务（比如XA 两阶段提交；应用、事务管理器(TM)、资源管理器(DB)），例如Atomikos。
2. 基于可靠消息服务的分布式事务（基于消息中间件）
3. 柔性事务TCC（Try-Confirm-Cancel）tcc-transaction
4. 最大努力通知，通过消息中间件向其他系统发送消息（重复投递+定期校对）



### 排序、翻页、函数计算问题

跨节点多库进行查询时，由于多库分表时，比如有两个节点，节点1 存的是奇数id=1,3,5,7,9……；节点2 存的是偶数id=2,4,6,8,10……，想获取id前十条数据，此时由于数据分别存到两个库中，执行select * from user_info order by id limit 0,10，无法达到实际需求。因此需要分别从两个库中执行该命令，并合并结果再取前十条数据。

### 全局主键避重问题

MySQL 的数据库里面字段有一个自增的属性，Oracle 也有Sequence 序列。如果是一个数据库，那么可以保证ID 是不重复的，但是水平分表以后，每个表都按照自己的规律自增，肯定会出现ID 重复的问题，这个时候我们就不能用本地自增的方式了。

**解决方案：**

1. UUID（Universally Unique Identifier 通用唯一识别码）UUID 标准形式包含32 个16 进制数字，分为5 段，形式为8-4-4-4-12 的36 个字符，例如：c4e7956c-03e7-472c-8909-d733803e79a9。

   > UUID 是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于UUID 非常长，会占用大量的存储空间；另外，作为主键建立索引和基于索引进行查询时都会存在性能问题，在InnoDB 中，UUID 的无序性会引起数据位置频繁变动，导致分页。

2. 数据库。把序号维护在数据库的一张表中。这张表记录了全局主键的类型、位数、起始值，当前值。当其他应用需要获得全局ID 时，先for update 锁行，取到值+1 后并且更新后返回。并发性比较差。

3. Redis。基于Redis 的INT 自增的特性，使用批量的方式降低数据库的写压力，每次获取一段区间的ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力。

4. 雪花算法Snowflake（64bit）。分为四段：

   a）使用41bit 作为毫秒数，可以使用69 年
   b）10bit 作为机器的ID（5bit 是数据中心，5bit 的机器ID），支持1024 个节点
   c）12bit 作为毫秒内的流水号（每个节点在每毫秒可以产生4096 个ID）
   d）最后还有一个符号位，永远是0。

   > 优点：毫秒数在高位，生成的ID 整体上按时间趋势递增；不依赖第三方系统，稳定性和效率较高，理论上QPS 约为409.6w/s(1000*2^12)，并且整个分布式系统内不会产生ID 碰撞；可根据自身业务灵活分配bit 位。
   >
   > 缺点：强依赖机器时钟，如果时钟回拨，则可能导致生成ID 重复。



## 多数据源

由于我们对数据进行了切分，数据分布在不同的节点上，那就意味着有多个数据源，以下是从客户端访问数据库服务的流程：

DAO—>Mapper（ORM）—>JDBC—>代理—>数据库服务

可以从流程各个节点解决多数据源问题：

### DAO 解决方案

在客户端DAO层，我们在连接到具体某一个数据源之前，可以做动态切换数据源。

Spring 中提供了一个抽象类 `AbstractRoutingDataSource`，可以实现数据源的动态切换。

步骤如下：

1. aplication.properties 定义多个数据源

   ```properties
   server.port=8082
   spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
   spring.datasource.driverClassName=com.mysql.cj.jdbc.Driver
   # 数据源1
   spring.datasource.druid.first.url=jdbc:mysql://localhost:3306/ds0?allowMultiQueries=true&useUnicode=true&characterEncoding=UTF-8&useSSL=false&serverTimezone=GMT%2B8
   spring.datasource.druid.first.username=root
   spring.datasource.druid.first.password=123456
   # 数据源2
   spring.datasource.druid.second.url=jdbc:mysql://localhost:3306/ds1?allowMultiQueries=true&useUnicode=true&characterEncoding=UTF-8&useSSL=false&serverTimezone=GMT%2B8
   spring.datasource.druid.second.username=root
   spring.datasource.druid.second.password=123456
   ```

2. 创建@TargetDataSource 注解

   ```java
   /**
    * 多数据源注解
    * 指定要使用的数据源
    */
   @Target(ElementType.METHOD)
   @Retention(RetentionPolicy.RUNTIME)
   @Documented
   public @interface TargetDataSource {
   
       String name() default "";
   
   }
   ```

3. 创建DynamicDataSource 继承AbstractRoutingDataSource

   ```java
   /**
    * 扩展Spring的 AbstractRoutingDataSource 抽象类，重写 determineCurrentLookupKey方法
    * 动态数据源
    * determineCurrentLookupKey() 方法决定使用哪个数据源
    *
    */
   public class DynamicDataSource extends AbstractRoutingDataSource {
   
       private static final ThreadLocal<String> CONTEXT_HOLDER = new ThreadLocal<>();
   
       /**
        * 决定使用哪个数据源之前需要把多个数据源的信息以及默认数据源信息配置好
        *
        * @param defaultTargetDataSource 默认数据源
        * @param targetDataSources       目标数据源
        */
       public DynamicDataSource(DataSource defaultTargetDataSource, Map<Object, Object> targetDataSources) {
           super.setDefaultTargetDataSource(defaultTargetDataSource);
           super.setTargetDataSources(targetDataSources);
           super.afterPropertiesSet();
       }
   
       @Override
       protected Object determineCurrentLookupKey() {
           return getDataSource();
       }
   
       public static void setDataSource(String dataSource) {
           CONTEXT_HOLDER.set(dataSource);
       }
   
       public static String getDataSource() {
           return CONTEXT_HOLDER.get();
       }
   
       public static void clearDataSource() {
           CONTEXT_HOLDER.remove();
       }
   
   }
   ```

4. 多数据源配置类DynamicDataSourceConfig

   ```java
   /**
    * 配置多数据源
    */
   @Configuration
   public class DynamicDataSourceConfig {
   
       @Bean
       @ConfigurationProperties("spring.datasource.druid.first")
       public DataSource firstDataSource(){
           return DruidDataSourceBuilder.create().build();
       }
   
       @Bean
       @ConfigurationProperties("spring.datasource.druid.second")
       public DataSource secondDataSource(){
           return DruidDataSourceBuilder.create().build();
       }
   
       @Bean
       @Primary
       public DynamicDataSource dataSource(DataSource firstDataSource, DataSource secondDataSource) {
           Map<Object, Object> targetDataSources = new HashMap<>(5);
           targetDataSources.put(DataSourceNames.FIRST, firstDataSource);
           targetDataSources.put(DataSourceNames.SECOND, secondDataSource);
           return new DynamicDataSource(firstDataSource, targetDataSources);
       }
   
   }
   ```

5. 创建切面类DataSourceAspect，对添加了@TargetDataSource 注解的类进行拦截设置数据源。

   ```java
   /**
    * 多数据源，切面处理类
    */
   @Slf4j
   @Aspect
   @Component
   public class DataSourceAspect implements Ordered {
   
       @Pointcut("@annotation(com.cavie.datasource.TargetDataSource)")
       public void dataSourcePointCut() {
   
       }
   
       @Around("dataSourcePointCut()")
       public Object around(ProceedingJoinPoint point) throws Throwable {
           MethodSignature signature = (MethodSignature) point.getSignature();
           Method method = signature.getMethod();
   
           TargetDataSource ds = method.getAnnotation(TargetDataSource.class);
           if (ds == null) {
               DynamicDataSource.setDataSource(DataSourceNames.FIRST);
               log.debug("set datasource is " + DataSourceNames.FIRST);
           } else {
               DynamicDataSource.setDataSource(ds.name());
               log.debug("set datasource is " + ds.name());
           }
   
           try {
               return point.proceed();
           } finally {
               DynamicDataSource.clearDataSource();
               log.debug("clean datasource");
           }
       }
   
       @Override
       public int getOrder() {
           return 1;
       }
   }
   ```

   

6. 在启动类上自动装配数据源配置@Import({DynamicDataSourceConfig.class})

   ```java
   @MapperScan("com.cavie.mapper")
   @SpringBootApplication(exclude = {DataSourceAutoConfiguration.class})
   @Import({DynamicDataSourceConfig.class})
   public class DynamicDSApp {
   
       public static void main(String[] args) {
           SpringApplication.run(DynamicDSApp.class, args);
       }
   
   }
   ```

7. 在实现类上加上注解， 如@TargetDataSource(name =DataSourceNames.SECOND)，调用



**优点：**不需要依赖ORM 框架，即使替换了ORM 框架也不受影响。实现简单（不需要解析SQL 和路由规则），可以灵活地定制。

**缺点：**不能复用，不能跨语言。



### ORM 解决方案

第二个是在框架层，比如我们用MyBatis 连接数据库，也可以指定数据源。我们可以基于MyBatis 插件的拦截机制（拦截query 和update 方法），实现数据源的选择。
例如：https://github.com/colddew/shardbatis
https://docs.jboss.org/hibernate/stable/shards/reference/en/html_single/



### JDBC 解决方案

不管是MyBatis 还是Hibernate，还是Spring 的JdbcTemplate，本质上都是对JDBC的封装，所以第三层就是驱动层。比如Sharding-JDBC，就是对JDBC 的对象进行了封装。JDBC 的核心对象：

* DataSource：数据源
* Connection：数据库连接
* Statement：语句对象
* ResultSet：结果集
  那我们只要对这几个对象进行封装或者拦截或者代理，就可以实现分片的操作。



### 代理层解决方案

前面三种都是在客户端实现的，也就是说不同的项目都要做同样的改动，不同的编程语言也有不同的实现，所以我们能不能把这种选择数据源和实现路由的逻辑提取出来，做成一个公共的服务给所有的客户端使用呢？这个就是第四层，代理层。比如 `Mycat` 和 `Sharding-Proxy`，都是属于这一层。



### 数据库服务解决方案

最后一层就是在数据库服务上实现，也就是服务层，某些特定的数据库或者数据库
的特定版本可以实现这个功能。



## Mycat

官网http://www.mycat.org.cn/
Mycat 概要介绍https://github.com/MyCATApache/Mycat-Server
入门指南https://github.com/MyCATApache/Mycat-doc/tree/master/%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97

### 概念

Mycat 属于代理层，运行在应用和数据库之间，可以当做一个MySQL 服务器使用，实现对MySQL
数据库的分库分表，也可以通过 JDBC 支持其他的数据库。（实际上就是应用和数据库之间的中间件）



特征：

1. 可以当做一个MySQL 数据库来使用
2. 支持MySQL 之外的数据库，通过JDBC 实现
3. 解决了我们提到的所有问题，多表join、分布式事务、全局序列号、翻页排序
4. 支持ZK 配置，带监控mycat-web



| 概念       | 含义                                                         |
| ---------- | ------------------------------------------------------------ |
| 主机       | 物理主机，一台服务器，一个数据库服务，一个3306 端口          |
| 物理数据库 | 真实的数据库                                                 |
| 物理表     | 真实的表                                                     |
| 分片       | 将原来单个数据库的数据切分后分散存储在不同的数据库节点       |
| 分片节点   | 分片以后数据存储的节点                                       |
| 分片键     | 分片依据的字段，例如order_info 表以id 为依据分片,id 就是分片键，通常是主键 |
| 分片算法   | 分片的规则，例如随机、取模、范围、哈希、枚举以及各种组合算法 |
| 逻辑表     | 相对于物理表，是分片表聚合后的结果，对于客户端来说跟真实的表没有区别 |
| 逻辑数据库 | 相对于物理数据库，是数据节点聚合后的结果                     |

通过 Mycat 可以管理多个数据源，并且实现了分库分表，生成逻辑库逻辑表使得对外客户端操作时像只操作单一数据库一样。



### 安装

linux下联网下载：

```sh
wget http://dl.mycat.io/1.6.7.3/20190927161129/Mycat-server-1.6.7.3-release-20190927161129-linux.tar.gz
tar -xzvf Mycat-server-1.6.7.3-release-20190927161129-linux.tar.gz
```



解压后目录如下：

| 目录    | 作用       |
| ------- | ---------- |
| bin     | 启动目录   |
| catlet  | 空目录     |
| conf    | 配置目录   |
| lib     | jar 包依赖 |
| logs 日 | 日志目录   |



### 配置

主要的配置文件server.xml、schema.xml、rule.xml 和具体的分片配置文件。

#### server.xml

系统配置信息

system 标签：例如字符集、线程数、心跳、分布式事务开关等等。
user 标签：配置登录用户和权限。

```xml
<mycat:server xmlns:mycat="http://io.mycat/">
	<system>
	<property name="nonePasswordLogin">0</property> <!-- 0为需要密码登陆、1为不需要密码登陆 ,默认为0，设置为1则需要指定默认账户-->
	<property name="ignoreUnknownCommand">0</property><!-- 0遇上没有实现的报文(Unknown command:),就会报错、1为忽略该报文，返回ok报文。
	在某些mysql客户端存在客户端已经登录的时候还会继续发送登录报文,mycat会报错,该设置可以绕过这个错误-->
	<property name="useHandshakeV10">1</property>
    <property name="removeGraveAccent">1</property>
	<property name="useSqlStat">0</property>  <!-- 1为开启实时统计、0为关闭 -->
	<property name="useGlobleTableCheck">0</property>  <!-- 1为开启全加班一致性检测、0为关闭 -->
	<property name="sqlExecuteTimeout">300</property>  <!-- SQL 执行超时 单位:秒-->
		<property name="sequenceHandlerType">1</property>
	<!--<property name="sequnceHandlerPattern">(?:(\s*next\s+value\s+for\s*MYCATSEQ_(\w+))(,|\)|\s)*)+</property>
	INSERT INTO `travelrecord` (`id`,user_id) VALUES ('next value for MYCATSEQ_GLOBAL',"xxx");
	-->
	<!--必须带有MYCATSEQ_或者 mycatseq_进入序列匹配流程 注意MYCATSEQ_有空格的情况-->
	<property name="sequnceHandlerPattern">(?:(\s*next\s+value\s+for\s*MYCATSEQ_(\w+))(,|\)|\s)*)+</property>
	<property name="subqueryRelationshipCheck">false</property> <!-- 子查询中存在关联查询的情况下,检查关联字段中是否有分片字段 .默认 false -->
	<property name="sequenceHanlderClass">io.mycat.route.sequence.handler.HttpIncrSequenceHandler</property>
      <!--  <property name="useCompression">1</property>--> <!--1为开启mysql压缩协议-->
        <!--  <property name="fakeMySQLVersion">5.6.20</property>--> <!--设置模拟的MySQL版本号-->
	<!-- <property name="processorBufferChunk">40960</property> -->
	<!-- 
	<property name="processors">1</property> 
	<property name="processorExecutor">32</property> 
	 -->
        <!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena | type 2 NettyBufferPool -->
		<property name="processorBufferPoolType">0</property>
		<!--默认是65535 64K 用于sql解析时最大文本长度 -->
		<!--<property name="maxStringLiteralLength">65535</property>-->
		<!--<property name="sequenceHandlerType">0</property>-->
		<!--<property name="backSocketNoDelay">1</property>-->
		<!--<property name="frontSocketNoDelay">1</property>-->
		<!--<property name="processorExecutor">16</property>-->
		<!--
			<property name="serverPort">8066</property>
			<property name="managerPort">9066</property>
			<property name="idleTimeout">300000</property>
			<property name="authTimeout">15000</property>
			<property name="bindIp">0.0.0.0</property>
			<property name="dataNodeIdleCheckPeriod">300000</property> 5 * 60 * 1000L; //连接空闲检查
			<property name="frontWriteQueueSize">4096</property> <property name="processors">32</property> -->
		<!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志-->
		<property name="handleDistributedTransactions">0</property>
		
		<!--off heap for merge/order/group/limit  1开启，0关闭-->
		<property name="useOffHeapForMerge">0</property>

		<!-- 单位为m -->
        <property name="memoryPageSize">64k</property>

		<!-- 单位为k -->
		<property name="spillsFileBufferSize">1k</property>

		<property name="useStreamOutput">0</property>

		<!-- 单位为m-->
		<property name="systemReserveMemorySize">384m</property>


		<!--是否采用zookeeper协调切换  -->
		<property name="useZKSwitch">false</property>

		<!-- XA Recovery Log日志路径 -->
		<!--<property name="XARecoveryLogBaseDir">./</property>-->

		<!-- XA Recovery Log日志名称 -->
		<!--<property name="XARecoveryLogBaseName">tmlog</property>-->
		<!--如果为 true的话 严格遵守隔离级别,不会在仅仅只有select语句的时候在事务中切换连接-->
		<property name="strictTxIsolation">false</property>
		<!--如果为0的话,涉及多个DataNode的catlet任务不会跨线程执行-->
		<property name="parallExecute">0</property>
		<property name="serverBacklog">2048</property>
	</system>
	
	<!-- 全局SQL防火墙设置 -->
	<!--白名单可以使用通配符%或着*-->
	<!--例如<host host="127.0.0.*" user="root"/>-->
	<!--例如<host host="127.0.*" user="root"/>-->
	<!--例如<host host="127.*" user="root"/>-->
	<!--例如<host host="1*7.*" user="root"/>-->
	<!--这些配置情况下对于127.0.0.1都能以root账户登录-->
	<!--
	<firewall>
	   <whitehost>
	      <host host="1*7.0.0.*" user="root"/>
	   </whitehost>
       <blacklist check="false">
       </blacklist>
	</firewall>
	-->

	<user name="root" defaultAccount="true">
		<property name="password">123456</property>
		<property name="schemas">TESTDB</property>
		<property name="defaultSchema">TESTDB</property>
		<!--No MyCAT Database selected 错误前会尝试使用该schema作为schema，不设置则为null,报错 -->
		
		<!-- 表级 DML 权限设置 -->
		<!-- 		
		<privileges check="false">
			<schema name="TESTDB" dml="0110" >
				<table name="tb01" dml="0000"></table>
				<table name="tb02" dml="1111"></table>
			</schema>
		</privileges>		
		 -->
	</user>

	<user name="user">
		<property name="password">user</property>
		<property name="schemas">TESTDB</property>
		<property name="readOnly">true</property>
		<property name="defaultSchema">TESTDB</property>
	</user>

</mycat:server>
```



#### schema.xml

schema 在MySQL 里面跟数据库是等价的。

schema.xml 包括逻辑库、表、分片规则、分片节点和数据源，可以定义多个schema。

主要有三个标签 `table`、`dataNode`、`dataHost`

> 表名和库名最好都用小写

**table：**定义了逻辑表，以及逻辑表分布的节点和分片规则：

```xml
<schema name="catmall" checkSQLschema="false" sqlMaxLimit="100">
    <!-- 范围分片-->
    <table name="customer" primaryKey="id" dataNode="dn1,dn2,dn3" rule="rang-long-cust" />
    <!-- 取模分片-->
    <table name="order_info" dataNode="dn1,dn2,dn3" rule="mod-long-order" >
        <!-- ER 表，通过joinkey、parentKey指定ER关联的键，使得相同的数据落在同一个节点上-->
        <childTable name="order_detail" primaryKey="id" joinKey="order_id" parentKey="order_id"/>
    </table>
    <!-- 全局表，指定节点都存储的表-->
    <table name="student" primaryKey="sid" type="global" dataNode="dn1,dn2,dn3" />
</schema>
```

| 参数          | 作用                                                         |
| ------------- | ------------------------------------------------------------ |
| primaryKey    | 指定该逻辑表对应真实表的主键。MyCat 会缓存主键（通过primaryKey 属性配置）与具体dataNode 的信息。当分片规则（rule）使用非主键进行分片时，那么在使用主键进行查询时，MyCat 就会通过缓存先确定记录在哪个dataNode 上，然后再在该dataNode 上执行查询。如果没有缓存/缓存并没有命中的话，还是会发送语句给所有的dataNode。 |
| dataNode      | 数据分片的节点                                               |
| autoIncrement | 自增长（全局序列），true 代表主键使用自增长策略              |
| type          | 全局表：global。其他：不配置                                 |



**dataNode：**

数据节点与物理数据库的对应关系。

```xml
<dataNode name="dn1" dataHost="host1" database="gpcat" />
```



**dataHost：**

配置物理主机的信息，readhost 是从属于writehost 的。

```xml
<dataHost name="host1" maxCon="1000" minCon="10" balance="0" writeType="0" dbType="mysql"
          dbDriver="native" switchType="1" slaveThreshold="100">
    <heartbeat>select user()</heartbeat>
    <!-- can have multi write hosts -->
    <writeHost host="hostM1" url="localhost:3306" user="root" password="123456">
        <!-- can have multi read hosts -->
        <readHost host="hostS2" url="192.168.8.146:3306" user="root" password="xxx"/>
    </writeHost>
    <writeHost host="hostS1" url="localhost:3316" user="root" password="123456"/>
    <!-- <writeHost host="hostM2" url="localhost:3316" user="root" password="123456"/> -->
</dataHost>
```

**balance：**负载的配置，决定select 语句的负载（读操作）

| 值   | 作用                                                         |
| ---- | ------------------------------------------------------------ |
| 0    | 不开启读写分离机制，所有读操作都发送到当前可用的writeHost 上。 |
| 1    | 所有读操作都随机发送到当前的writeHost 对应的readHost 和备用的writeHost |
| 2    | 所有的读操作都随机发送到所有的writeHost,readHost 上          |
| 3    | 所有的读操作都只发送到writeHost 的readHost 上                |

**writeType：**读写分离的配置，决定update、delete、insert 语句的负载（写操作）

| 值   | 作用                                                         |
| ---- | ------------------------------------------------------------ |
| 0    | 所有写操作都发送到可用的writeHost 上（默认第一个，第一个挂了以后发到第二个） |
| 1    | 所有写操作都随机的发送到writeHost                            |

**switchType：**主从切换配置

| 值   | 作用                                                         |
| ---- | ------------------------------------------------------------ |
| -1   | 表示不自动切换                                               |
| 1    | 默认值，表示自动切换                                         |
| 2    | 基于MySQL 主从同步的状态决定是否切换,心跳语句为show slave status |
| 3    | 基于MySQL galary cluster 的切换机制（适合集群）（1.4.1），心跳语句为show status like 'wsrep%'。 |



#### rule.xml

定义了分片规则和算法

分片规则即对应的分片算法

```xml
<tableRule name="rang-long-cust">
    <rule>
        <columns>id</columns>
        <algorithm>func-rang-long-cust</algorithm>
    </rule>
</tableRule>
<function name="func-rang-long-cust" class="io.mycat.route.function.AutoPartitionByLong">
    <property name="mapFile">rang-long-cust.txt</property>
</function>
```

分片配置：rang-long-cust.txt（将0-1w的id划分到节点0,1w-2w的id划分到节点1...）

```txt
0-10000=0
10001-20000=1
20001-100000=2
```



#### ZK配置

https://www.cnblogs.com/leeSmall/p/9551038.html

Mycat 也支持ZK 配置（ 用于管理配置和生成全局ID ） ， 执行bin 目录下init_zk_data.sh,会自动将zkconf 下的所有配置文件上传到ZK（先拷贝到这个目录）。

```sh
cd /usr/local/soft/mycat/conf
cp *.txt *.xml *.properties zkconf/
cd /usr/local/soft/mycat/bin
./init_zk_data.sh
```

修改mycat/conf/myid.properties，将ZK配置启用

```properties
loadZk=true
zkURL=127.0.0.1:2181
clusterId=010
myid=01001
clusterSize=1
clusterNodes=mycat_gp_01
#server booster ; booster install on db same server,will reset all minCon to 2
type=server
boosterDataHosts=dataHost1
```

注意如果执行init_zk_data.sh 脚本报错的话，代表未写入成功，此时不要启用ZK配置并重启，否则本地文件会被覆盖。启动时如果loadzk=true 启动时，会自动从zk 下载配置文件覆盖本地配置。在这种情况下如果修改配置，需要先修改conf 目录的配置，copy 到zkconf，再执行上传。



### 启动停止

进入mycat/bin 目录（注意要先启动物理数据库）：

| 操作     | 命令            |
| -------- | --------------- |
| 启动     | ./mycat start   |
| 停止     | ./mycat stop    |
| 重启     | ./mycat restart |
| 查看状态 | ./mycat status  |
| 前台运行 | ./mycat console |

启动连接

```sh
mysql -uroot -p123456 -h 192.168.8.151 -P8066 catmall
```



### 全局ID

Mycat 全局序列实现方式主要有4 种：本地文件方式、数据库方式、本地时间戳算法、ZK。也可以自定义业务序列。

> 注意获取全局ID 的前缀都是：MYCATSEQ_

在schema.xml 的table 标签上配置autoIncrement="true"，不需要获取和指定序列的情况下，就可以使用全局ID 了。

#### 本地文件方式

配置文件server.xml sequnceHandlerType 值：

```xml
<!-- 0 文件1 数据库2 本地时间戳3 ZK -->
<property name="sequnceHandlerType">0</property>
```

配置conf/sequence_conf.properties

```properties
CUSTOMER.HISIDS=
CUSTOMER.MINID=10000001
CUSTOMER.MAXID=20000000
CUSTOMER.CURID=10000001
```

语法：select next value for MYCATSEQ_CUSTOMER（查询CUSTOMER下一个全局id）

```sql
INSERT INTO `customer` (`id`, `name`) VALUES (next value for MYCATSEQ_CUSTOMER, 'user');
```

**优点：**本地加载，读取速度较快。
**缺点：**当Mycat 重新发布后，配置文件中的sequence 需要替换。Mycat 不能做集群部署。

#### 数据库方式

配置文件server.xml sequnceHandlerType 值：

 ```xml
 <property name="sequnceHandlerType">1</property>
 ```

配置： sequence_db_conf.properties
把这张表创建在指定数据库节点ds1上

```properties
#sequence stored in datanode
GLOBAL=ds1
CUSTOMER=ds1
```

在指定的数据库节点上创建MYCAT_SEQUENCE 表：

```sql
DROP TABLE IF EXISTS MYCAT_SEQUENCE;
CREATE TABLE MYCAT_SEQUENCE (
name VARCHAR(50) NOT NULL,
current_value INT NOT NULL,
increment INT NOT NULL DEFAULT 1,
remark varchar(100),
PRIMARY KEY(name)) ENGINE=InnoDB;
```

在schema.xml 配置文件中配置这张表，供外部访问。

```xml
<table name="mycat_sequence" dataNode="dn1" autoIncrement="true" primaryKey="id"></table>
```

创建存储过程——获取当前sequence 的值

```sql
DROP FUNCTION IF EXISTS `mycat_seq_currval`;
DELIMITER ;;
CREATE DEFINER=`root`@`%` FUNCTION `mycat_seq_currval`(seq_name VARCHAR(50)) RETURNS varchar(64)
CHARSET latin1
DETERMINISTIC
BEGIN
DECLARE retval VARCHAR(64);
SET retval="-999999999,null";
SELECT concat(CAST(current_value AS CHAR),",",CAST(increment AS CHAR) ) INTO retval FROM
MYCAT_SEQUENCE WHERE name = seq_name;
RETURN retval ;
END
;;
DELIMITER ;
```

创建存储过程，获取下一个sequence

```sql
DROP FUNCTION IF EXISTS `mycat_seq_nextval`;
DELIMITER ;;
CREATE DEFINER=`root`@`%` FUNCTION `mycat_seq_nextval`(seq_name VARCHAR(50)) RETURNS varchar(64)
CHARSET latin1
DETERMINISTIC
BEGIN
UPDATE MYCAT_SEQUENCE
SET current_value = current_value + increment WHERE name = seq_name;
RETURN mycat_seq_currval(seq_name);
END
;;
DELIMITER ;
```

创建存储过程，设置sequence

```sql
DROP FUNCTION IF EXISTS `mycat_seq_setval`;
DELIMITER ;;
CREATE DEFINER=`root`@`%` FUNCTION `mycat_seq_setval`(seq_name VARCHAR(50), value INTEGER)
RETURNS varchar(64) CHARSET latin1
DETERMINISTIC
BEGIN
UPDATE MYCAT_SEQUENCE
SET current_value = value
WHERE name = seq_name;
RETURN mycat_seq_currval(seq_name);
END
;;
DELIMITER ;
```

插入需要自增的信息记录

```sql
INSERT INTO MYCAT_SEQUENCE(name,current_value,increment,remark) VALUES ('GLOBAL', 1, 100,'全局表使用');
INSERT INTO MYCAT_SEQUENCE(name,current_value,increment,remark) VALUES ('ORDERS', 1, 100,'订单表使用');
```

测试语句

```sql
select next value for MYCATSEQ_ORDERS
```



#### 本地时间戳算法

类似于雪花算法。

ID= 64 位二进制(42(毫秒)+5(机器ID)+5(业务编码)+12(重复累加) ，长度为18 位

配置文件server.xml sequnceHandlerType 值：

```xml
<property name="sequnceHandlerType">2</property>
```

配置文件sequence_time_conf.properties

```properties
#sequence depend on TIME
WORKID=01
DATAACENTERID=01
```

验证语句

```sql
select next value for MYCATSEQ_GLOBAL
```



#### ZK方式

修改conf/myid.properties，设置loadZk=true（启动时会从ZK 加载配置，一定要注意备份配置文件，并且先用bin/init_zk_data.sh,把配置文件写入到ZK）

配置文件server.xml sequnceHandlerType 值：

```xml
<property name="sequnceHandlerType">3</property>
```

配置文件：sequence_distributed_conf.properties

```properties
# 代表使用zk
INSTANCEID=ZK
# 与myid.properties 中的CLUSTERID 设置的值相同
CLUSTERID=010
```

复制配置文件

```sh
cd /usr/local/soft/mycat/conf
cp *.txt *.xml *.properties zkconf/
chown -R zkconf/
cd /usr/local/soft/mycat/bin
./init_zk_data.sh
```

验证语句

```sql
select next value for MYCATSEQ_GLOBAL
```



### 监控

#### 命令行监控

连接到管理端口9066，注意必须要带IP

```sh
mysql -uroot -h127.0.0.1 -p123456 -P9066
```

相关命令：

| 命令              | 作用                                                         |
| ----------------- | ------------------------------------------------------------ |
| show @@server     | 查看服务器状态，包括占用内存等                               |
| show @@database   | 查看数据库                                                   |
| show @@datanode   | 查看数据节点                                                 |
| show @@datasource | 查看数据源                                                   |
| show @@connection | 该命令用于获取Mycat 的前端连接状态，即应用与mycat 的连接     |
| show @@backend    | 查看后端连接状态                                             |
| show @@cache      | 查看缓存使用情况<br/>SQLRouteCache：sql 路由缓存。<br/>TableID2DataNodeCache ： 缓存表主键与分<br/>片对应关系。<br/>ER_SQL2PARENTID ：缓存ER 分片中子表与<br/>父表关系 |
| reload @@config   | 重新加载基本配置，使用这个命令时mycat服务不可用              |
| show @@sysparam   | 查看参数                                                     |
| show @@sql.high   | 执行频率高的SQL                                              |
| show @@sql.slow   | 慢SQL。设置慢SQL 的命令：reload @@sqlslow=5 ;                |



#### Web监控

https://github.com/MyCATApache/Mycat-download/tree/master/mycat-web-1.0

Mycat-eye 是mycat 提供的一个监控工具，它依赖于ZK。本地必须要运行一个ZK，必须先启动ZK。

下载mycat-web

```sh
cd /usr/local/soft
wget http://dl.mycat.io/mycat-web-1.0/Mycat-web-1.0-SNAPSHOT-20170102153329-linux.tar.gz
tar -xzvf Mycat-web-1.0-SNAPSHOT-20170102153329-linux.tar.gz
```

启动mycat-web

```sh
cd mycat-web
nohup ./start.sh &
```

停止：kill start.jar 相关的进程

访问端口8082，如http://192.168.8.151:8082/mycat/

修改mycat的server.xml 配置

```xml
<!-- 1 为开启实时统计、0 为关闭-->
<property name="useSqlStat">1</property>
```

重启mycat 服务生效



### 日志

log4j配置文件的level 配置要改成debug。主要日志如下：

**wrapper.log ：**
mycat 启动，停止，添加为服务等都会记录到此日志文件，如果系统环境配置错误或缺少配置时，导致Mycat 无法启动，可以通过查看wrapper.log 定位具体错误原因。

**mycat.log ：**
记录了启动时分配的相关buffer 信息，数据源连接信息，连接池，动态类加载信息等等。在conf/log4j2.xml 文件中进行相关配置，如保留个数，大小，字符集，日志文件大小等。



### Mycat注解

**注解的作用：**Mycat 在执行SQL 之前会先解析SQL 语句，在获得分片信息后再到对应的物理节点上执行。如果SQL 语句无法解析，则不能被执行。**如果语句中有注解，则会先解析注解的内容获得分片信息，再把真正需要执行的SQL 语句发送对对应的物理节点上。**

> 如果注解没有使用正确的条件，会导致原始SQL 被发送到所有的节点上执行，造成数据错误。



**注解解决问题：**

* 当关联的数据不在同一个节点的时候，Mycat 是无法实现跨库join 的。
* Mycat 不支持的SQL 语句，比如存储过程。

Mysql本身不支持执行创表、存储过程等操作，但其可以通过Mycat提供的注解，路由到指定的物理节点去执行。



#### 注解用法

注解的使用形式:

```sql
/*!mycat: sql=注解SQL 语句*/
真正执行的SQL
```

> 使用时将= 号后的"注解SQL 语句" 替换为需要的SQL 语句即可。

注解使用注意点（Mycat注解的限制）：

| 真正执行的sql | 注解sql                                                      |
| ------------- | ------------------------------------------------------------ |
| select        | 如果需要确定分片，则使用能确定分片的注解，比如/*!mycat: sql=select * from users where user_id=1*/<br/>如果要在所有分片上执行则可以不加能确定分片的条件 |
| insert        | 使用insert 的表作为注解SQL，必须能确定到某个分片<br/>原始SQL 插入的字段必须包括分片字段<br/>非分片表（只在某个节点上）：必须能确定到某个分片 |
| delete        | 使用delete 的表作为注解SQL                                   |
| update        | 使用update 的表作为注解SQL                                   |

使用注解并不额外增加MyCat 的执行时间；从解析复杂度以及性能考虑，注解SQL 应尽量简单，因为它只是用来做路由的。

案例：

**创建表或存储过程：customer.id=1 全部路由到数据库1**

```sql
-- 存储过程
/*!mycat: sql=select * from customer where id =1 */ 
CREATE PROCEDURE test_proc() BEGIN END ;
-- 表
/*!mycat: sql=select * from customer where id =1 */ 
CREATE TABLE test2(id INT);
```



**转发insert语句到指定节点：**

```sql
/*!mycat: sql=select * from customer where id =1 */ 
INSERT INTO test2(id) SELECT id FROM order_detail;
```



**关联查询ShareJoin：**

```sql
/*!mycat:catlet=io.mycat.catlets.ShareJoin */
select a.order_id,b.price from order_info a, order_detail b where a.nums = b.goods_id;
```

> ShareJoin 原理如下：首先会到所有节点查询a.nums的值，然后到所有节点查询b.goods_id的值，接着汇总一起，进行关联查询匹配。
>
> 注意：查询的两张表的字段不能是分片关键字段（即上述的a.nums和b.goods_id），因为如果是分片的关键字段，则会先根据分片规则，选出该值所在的节点，然后查询b表的语句也会路由到所选的节点去执行。例如a.nums的值1路由到节点1去查询，那么b.goods_id的值1也会路由到节点1去查询，但实际上b.goods_id的值1在节点2上，所以会导致关联查询失败。



**读写分离：**

配置Mycat 读写分离后，默认查询都会从读节点获取数据，但是有些场景需要获取实时数据，如果从读节点获取数据可能因延时而无法实现实时，**Mycat 支持通过注解`/*balance*/ `来强制从写节点（write host）查询数据**。

```sql
/*balance*/ 
select a.* from customer a where a.id=6666;
```

1.6 版本之后新增了指定读写数据库选择注解：

```sql
/*!mycat: db_type=master */ select * from customer;
/*!mycat: db_type=slave */ select * from customer;
-- #这种不能再Mybatis识别，只能直接在数据库运行
/*#mycat: db_type=master */ select * from customer;
/*#mycat: db_type=slave */ select * from customer;
```



### 分片策略

**分片的作用**是将大量数据和访问请求均匀分布在多个节点上，通过这种方式提升数据服务的存储和负载能力。

总体上分为**连续分片**和**离散分片**，还有一种是连续分片和离散分片的结合，例如先范围后取模。



#### 切分规则选择

1. 找到需要切分的大表，和关联的表
2. 确定分片字段（尽量使用主键），一般用最频繁使用的查询条件
3. 考虑单个分片的存储容量和请求、数据增长（业务特性）、扩容和数据迁移问题。

> 一般来说，分片数要比当前规划的节点数要大。

#### 连续分片

连续分片，单个分区的数量和边界是确定的。

**优点：**

1. 范围条件查询消耗资源少（不需要汇总数据）
2. 扩容无需迁移数据（分片固定）

**缺点：**

1. 存在数据热点的可能性（例如经常访问的数据集中在某一个分区）
2. 并发访问能力受限于单一或少量DataNode（访问集中）



##### 范围分片

特点：容易出现冷热数据

分片规则rule.xml

```xml
<tableRule name="auto-sharding-long">
    <rule>
        <columns>id</columns>
        <algorithm>rang-long</algorithm>
    </rule>
</tableRule>
<function name="rang-long" class="io.mycat.route.function.AutoPartitionByLong">
    <property name="mapFile">autopartition-long.txt</property>
</function>
```

autopartition-long.txt

```txt
# range start-end ,data node index
# K=1000,M=10000.
0-500M=0
500M-1000M=1
1000M-1500M=2
```



##### 按自然月分片

建表语句

```sql
CREATE TABLE `sharding_by_month` (
    `create_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP,
    `db_nm` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

逻辑表，Schema.xml

```xml
<schema name="catmall" checkSQLschema="false" sqlMaxLimit="100">
    <table name="sharding_by_month" dataNode="dn1,dn2,dn3" rule="qs-sharding-by-month" />
</schema>
```

分片规则，rule.xml

```xml
<tableRule name="sharding-by-month">
    <rule>
        <columns>create_time</columns>
        <algorithm>qs-partbymonth</algorithm>
    </rule>
</tableRule>
<function name="qs-partbymonth" class="io.mycat.route.function.PartitionByMonth">
    <property name="dateFormat">yyyy-MM-dd</property>
    <property name="sBeginDate">2020-10-01</property>
    <property name="sEndDate">2020-12-31</property>
</function>
```

> columns 标识将要分片的表字段，字符串类型，与dateFormat 格式一致。
> algorithm 为分片函数。
> dateFormat 为日期字符串格式。
> sBeginDate 为开始日期。
> sEndDate 为结束日期
> 注意：节点个数要大于月份的个数

#### 离散分片

离散分片的分区总数量和边界是确定的。



**优点：**

1. 并发访问能力增强（负载到不同的节点）
2. 范围条件查询性能提升（并行计算）



**缺点：**

1. 数据扩容比较困难，涉及到数据迁移问题（数据均匀分布在各个节点，扩容后需要重新将数据分区）
2. 数据库连接消耗比较多（查询的数据可能在许多节点上，因此要连接大量数据库）



##### 枚举分片

将所有可能出现的值列举出来，指定分片。例如：全国34 个省，要将不同的省的数据存放在不同的节点，可用枚举的方式。适用于枚举值固定的场景。

建表语句：

```sql
CREATE TABLE `sharding_by_intfile` (
    `age` int(11) NOT NULL,
    `db_nm` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

逻辑表，Schema.xml：

```xml
<table name="sharding_by_intfile" dataNode="dn$1-3" rule="qs-sharding-by-intfile" />
```

分片规则，rule.xml：

```xml
<tableRule name="sharding-by-intfile">
    <rule>
        <columns>sharding_id</columns>
        <algorithm>hash-int</algorithm>
    </rule>
</tableRule>
<function name="hash-int" class="org.opencloudb.route.function.PartitionByFileMap">
    <property name="mapFile">partition-hash-int.txt</property>
    <property name="type">0</property>
    <property name="defaultNode">0</property>
</function>
```

> type：默认值为0，0 表示Integer，非零表示String。
> PartitionByFileMap.java，通过map 来实现。

策略文件：partition-hash-int.txt（=号左边值，右边为路由的节点）

```txt
1=0
2=1
3=2
```



##### 一致性哈希

一致性hash 有效解决了分布式数据的扩容问题。

建表语句：

```sql
CREATE TABLE `sharding_by_murmur` (
    `id` int(10) DEFAULT NULL,
    `db_nm` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

逻辑表，schema.xml：

```xml
<schema name="test" checkSQLschema="false" sqlMaxLimit="100">
    <table name="sharding_by_murmurhash" primaryKey="id" dataNode="dn$1-3" rule="sharding-by-murmur" />
</schema>
```

分片规则，rule.xml：

```xml
<tableRule name="sharding-by-murmur">
    <rule>
        <columns>id</columns>
        <algorithm>qs-murmur</algorithm>
    </rule>
</tableRule>
<function name="qs-murmur" class="io.mycat.route.function.PartitionByMurmurHash">
    <property name="seed">0</property>
    <property name="count">3</property>
    <property name="virtualBucketTimes">160</property>
</function>
```



##### 取模

特点：分布均匀，但是迁移工作量比较大

分片规则，rule.xml：

```xml
<tableRule name="mod-long">
    <rule>
        <columns>sid</columns>
        <algorithm>mod-long</algorithm>
    </rule>
</tableRule>
<function name="mod-long" class="io.mycat.route.function.PartitionByMod">
    <!-- how many data nodes -->
    <property name="count">3</property>
</function>
```



##### 固定分片哈希

这是先求模得到逻辑分片号，再根据逻辑分片号直接映射到物理分片的一种散列算法。（取模+固定范围）

建表语句：

```sql
CREATE TABLE `sharding_by_long` (
    `id` int(10) DEFAULT NULL,
    `db_nm` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

逻辑表，schema.xml：

```xml
<schema name="test" checkSQLschema="false" sqlMaxLimit="100">
    <table name="sharding_by_long" dataNode="dn$1-3" rule="qs-sharding-by-long" />
</schema>
```

分片规则，rule.xml：

```xml
<tableRule name="qs-sharding-by-long">
    <rule>
        <columns>id</columns>
        <algorithm>qs-sharding-by-long</algorithm>
    </rule>
</tableRule>
<function name="qs-sharding-by-long" class="io.mycat.route.function.PartitionByLong">
    <property name="partitionCount">8</property>
    <property name="partitionLength">128</property>
</function>
```

> * partitionCount 为指定分片个数列表。（控制有多少个分片）
> * partitionLength 为分片范围列表。（每个分片的数据量）
>
> 上面表示一共能存8*128条数据

可以对分区数量和分区范围进行多个值，例如：

```xml
<function name="qs-sharding-by-long" class="io.mycat.route.function.PartitionByLong">
    <property name="partitionCount">2,1</property>
    <property name="partitionLength">256,512</property>
</function>
```

上面是一个有三个分区，前两个分区范围长度为256，第三个为512。



##### 取模范围分片

建表语句：

```sql
CREATE TABLE `sharding_by_pattern` (
    `id` varchar(20) DEFAULT NULL,
    `db_nm` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```



逻辑表，schema.xml：

```xml
<schema name="test" checkSQLschema="false" sqlMaxLimit="100">
    <table name="sharding_by_pattern" primaryKey="id" dataNode="dn$0-10" rule="qs-sharding-by-pattern" />
</schema>
```



分片规则，rule.xml：

```xml
<tableRule name="sharding-by-pattern">
    <rule>
        <columns>user_id</columns>
        <algorithm>sharding-by-pattern</algorithm>
    </rule>
</tableRule>
<function name="sharding-by-pattern" class=" io.mycat.route.function.PartitionByPattern">
    <property name="patternValue">100</property>
    <property name="defaultNode">0</property>
    <property name="mapFile">partition-pattern.txt</property>
</function>
```

> patternValue 取模基数，这里设置成100

partition-pattern.txt（取模后的余数按照范围路由到不同节点）

```txt
# id partition range start-end ,data node index
###### first host configuration
1-20=0
21-70=1
71-100=2
0-0=0
```



##### 范围取模分片

每个范围按照指定的节点数取模。扩容的时候旧数据无需迁移。

建表语句：

```sql
CREATE TABLE `sharding_by_rang_mod` (
    `id` bigint(20) DEFAULT NULL,
    `db_nm` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```



逻辑表，schema.xml：

```xml
<schema name="test" checkSQLschema="false" sqlMaxLimit="100">
    <table name="sharding_by_rang_mod" dataNode="dn$1-3" rule="qs-sharding-by-rang-mod" />
</schema>
```



分片规则，rule.xml：

```xml
<tableRule name="qs-sharding-by-rang-mod">
    <rule>
        <columns>id</columns>
        <algorithm>qs-rang-mod</algorithm>
    </rule>
</tableRule>
<function name="qs-rang-mod" class="io.mycat.route.function.PartitionByRangeMod">
    <property name="mapFile">partition-range-mod.txt</property>
</function>
```

partition-range-mod.txt

```txt
# range start-end ,data node group size
0-20000=1
20001-40000=2
```

上面0-2w放在节点1上，2w-4w放在节点2、3上（取模2均分到这两个节点）。

> 节点数不能多余实际物理节点数



##### 自定义分片规则

自定义规则要类要继承 `AbstractPartitionAlgorithm` 并实现 `RuleAlgorithm` 接口。

然后再rule.xml指定使用的分片规则类名



### Mycat 离线扩容

当我们规划了数据分片，而数据已经超过了单个节点的存储上限，或者需要下线节点的时候，就需要对数据重新分片。



#### Mycat自带工具

以下例子的表以取模分片规则为例。

**准备工作：**

1. mycat 所在环境安装mysql 客户端程序。
2. mycat 的lib 目录下添加mysql 的jdbc 驱动包。
3. 对扩容缩容的表所有节点数据进行备份，以便迁移失败后的数据恢复。

**步骤：**

1. 复制schema.xml、rule.xml 并重命名为newSchema.xml、newRule.xml 放于conf 目录下。
2. 修改newSchema.xml 和newRule.xml 配置文件为扩容缩容后的mycat配置参数（表的节点数、数据源、路由规则）。

> 注意：只有节点变化的表才会进行迁移。仅分片配置变化不会迁移。

newSchema.xml，修改扩容的表的dataNode节点数量

```xml
<table name="sharding_by_mod" dataNode="dn1,dn2" rule="qs-sharding-by-mod" />
```

newRule.xml 修改count 个数

```xml
<function name="qs-sharding-by-mod-long" class="io.mycat.route.function.PartitionByMod">
    <property name="count">2</property>
</function>
```

3. 修改conf 目录下的migrateTables.properties 配置文件，告诉工具哪些表需要进行扩容或缩容，没有出现在此配置文件的schema 表不会进行数据迁移，格式：

   > 注意：
   >
   > * 不迁移的表，不要修改dn 个数，否则会报错。
   > * 如果是ER 表，因为只有主表有分片规则，所以子表不会迁移。

   ```properties
   逻辑库=逻辑表名
   ```

4. 修改 dataMigrate.sh 配置

   通过命令"find / -name mysqldump" 查找mysqldump 路径为"/usr/bin/mysqldump"，指定#mysql bin 路径为"/usr/bin/"

   ```sh
   #mysql bin 路径
   RUN_CMD="$RUN_CMD -mysqlBin= /usr/bin/"
   ```

5. 停止mycat 服务
6. 执行执行bin/ dataMigrate.sh 脚本
   注意：必须要配置Java 环境变量，不能用openjdk
7. 脚本执行完成， 如果最后的数据迁移验证通过， 就可以将之前的newSchema.xml 和newRule.xml 替换之前的schema.xml 和rule.xml 文件，并重启mycat 即可。

**注意事项：**

1. 保证分片表迁移数据前后路由规则一致（取模——取模）。
2. 保证分片表迁移数据前后分片字段一致。
3. 全局表将被忽略。
4. 不要将非分片表配置到migrateTables.properties 文件中。
5. 暂时只支持分片表使用MySQL 作为数据源的扩容缩容。



#### mysqldump 方式

系统第一次上线，把单张表迁移到Mycat，也可以用mysqldump。

MySQL 导出

```sh
mysqldump -uroot -p123456 -h127.0.0.1 -P3306 -c -t --skip-extended-insert mydatabase > mysql-1017.sql
```

> -c 表示带列名
> -t 表示只要数据，不要建表语句
> --skip-extended-insert 表示生成多行insert（mycat childtable 不支持多行插入）

Mycat 导入

```sh
mysql -uroot -p123456 -h127.0.0.1 -P8066 catmall < mysql-1017.sql
```

Mycat 导出

```sh
mysqldump -h192.168.8.151 -uroot -p123456 -P8066 -c -t --skip-extended-insert catmall customer > mycat-cust.sql
```

其他导入方式：

```sh
load data local infile '/mycat/customer.txt' into table customer;
# 或者
source sql '/mycat/customer.sql';
```



### Mycat核心总结

![Mycat核心流程](https://raw.githubusercontent.com/Cavielee/notePics/main/Mycat核心流程.jpg)

**启动流程：**

1. MycatServer 启动，解析配置文件，包括服务器、分片规则等
2. 创建工作线程，建立前端连接和后端连接

**执行SQL流程：**

1. 前端连接接收MySQL 命令

2. 解析MySQL，Mycat 用的是Druid 的DruidParser

3. 获取路由

4. 改写MySQL，例如两个条件在两个节点上，则变成两条单独的SQL

   > 例如：select * from customer where id in(5000001, 10000001);
   > 改写成：
   > select * from customer where id = 5000001；（dn2 执行）
   > select * from customer where id = 10000001；（dn3 执行）
   >
   > 又比如多表关联查询，先到各个分片上去获取结果，然后在内存中计算

5. 与数据库建立连接
6. 发送SQL 语句到MySQL 执行
7. 获取返回结果
8. 处理返回结果，例如排序、计算等等
9. 返回给客户端



## Sharding-JDBC

Sharding-JDBC 是多数据源、分库分表等问题在 JDBC 层面上的一种解决方案。实际上他对 JDBC 中核心对象 `DataSource`、`Connection`、`Statement`、`ResultSet` 进行封装。

Sharding-JDBC 也推出 Sharding-Sphere 作为 Mycat 的对标，Sharding-Sphere是代理层的实现。



### 定义

Sharding-JDBC 为轻量级Java 框架，在Java 的JDBC 层提供的额外服务。它使用客户端直连数据库，以jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC 驱动，完全兼容JDBC 和各种ORM 框架。

因此原本使用Mybatis的应用，无需修改原来代码，只需要添加相关依赖和对应的分片规则（配置/代码形式）即可。



### 工作流程

Sharding-JDBC 的原理总结起来很简单：

#### 1. SQL 解析

主要是词法和语法的解析。目前常见的SQL 解析器主要有fdb，jsqlparser和Druid。Sharding-JDBC1.4.x 之前的版本使用Druid 作为SQL 解析器。从1.5.x 版本开始，Sharding-JDBC 采用完全自研的SQL 解析引擎。



#### 2. 执行器优化



#### 3. SQL 路由

![路由引擎结构](https://shardingsphere.apache.org/document/current/img/sharding/route_architecture.png)

SQL 路由是根据分片规则配置以及解析上下文中的分片条件，将SQL 定位至真正的数据源。它又分为直接路由、简单路由和笛卡尔积路由。

* 直接路由：Hint 方式，直接指定了路由到那个节点。
* 简单路由：通过分片规则，会根据分片键的值根据分片算法落到唯一的节点上（绑定表指ER表，两个表遵循相同的分片规则，因此同一个值一定会落在同一个节点上）。
* 笛卡尔积路由：关联查询的表，没有使用分片键去进行路由，无法确定路由到那个节点。因此会将关联查询发到所有节点执行。（加入关联表1和表2有两条数据，则会一个节点会生成4条数据，2*2。那么两个节点就会有8条数据）

#### 4. SQL 改写

例如：将逻辑表名称改成真实表名称，优化分页查询等

#### 5. SQL 执行

因为可能链接到多个真实数据源， Sharding -JDBC 将采用多线程并发执行SQL。

#### 6. 结果归并

例如数据的组装、分页、排序等等。



### 快速使用

#### 引入依赖：

```xml
<dependency>
    <groupId>io.shardingsphere</groupId>
    <artifactId>sharding-jdbc-spring-boot-starter</artifactId>
    <version>3.1.0</version>
</dependency>
```



#### 定义数据源和分片策略

##### Java 代码配置

把数据源和分片策略都写在Java Config 中。

**优点：**灵活，可以实现各种定义的分片策略。

**缺点：**数据源、策略都配置在Java Config中，就出现了硬编码，在修改的时候比较麻烦。



```java
/**
 * 分片数据源配置，返回 ShardingDataSource
 */
@Configuration
@MapperScan(basePackages = "com.cavie.mapper", sqlSessionFactoryRef = "sqlSessionFactory")
public class DataSourceConfig {
    @Bean
    @Primary
    public DataSource shardingDataSource() throws SQLException {
        ShardingRuleConfiguration src = new ShardingRuleConfiguration();
        // 默认的分库策略
        src.setDefaultDatabaseShardingStrategyConfig(new StandardShardingStrategyConfiguration("user_id", DBShardAlgo.class.getName()));
        // 默认的分表策略
        src.setDefaultTableShardingStrategyConfig(new StandardShardingStrategyConfiguration("user_id", TblPreShardAlgo.class.getName(),TblRangeShardAlgo.class.getName()));
        // 为user_info表设置分库分表策略、算法
        src.getTableRuleConfigs().add(getUserTableRuleConfiguration());
        // 数据源名和数据源的映射表
        return new ShardingDataSource(src.build(createDataSourceMap()));
    }

    // 配置数据源
    private Map<String, DataSource> createDataSourceMap() {
        Map<String, DataSource> result = new HashMap<>();
        result.put("ds0", createDataSource("jdbc:mysql://localhost:3306/ds0?characterEncoding=utf8&useSSL=false&serverTimezone=UTC"));
        result.put("ds1", createDataSource("jdbc:mysql://localhost:3306/ds1?characterEncoding=utf8&useSSL=false&serverTimezone=UTC"));
        return result;
    }

    // 根据数据源地址创建 DataSource
    private DataSource createDataSource(final String dataSourceName) {
        BasicDataSource result = new BasicDataSource();
        result.setDriverClassName("com.mysql.cj.jdbc.Driver");
        result.setUrl(dataSourceName);
        result.setUsername("root");
        result.setPassword("123456");
        return result;
    }

    // 事务管理器
    @Bean
    public DataSourceTransactionManager transactionManager(DataSource shardingDataSource) {
        return new DataSourceTransactionManager(shardingDataSource);
    }

    // 为user_info表设置分库分表策略、算法
    public TableRuleConfiguration getUserTableRuleConfiguration() {
        TableRuleConfiguration userTableRuleConfig=new TableRuleConfiguration();
        userTableRuleConfig.setLogicTable("user_info");
        userTableRuleConfig.setActualDataNodes("ds0.user_info, ds1.user_info");
        userTableRuleConfig.setDatabaseShardingStrategyConfig(new StandardShardingStrategyConfiguration("user_id", DBShardAlgo.class.getName()));
        userTableRuleConfig.setTableShardingStrategyConfig(new StandardShardingStrategyConfiguration("user_id",TblPreShardAlgo.class.getName(), TblRangeShardAlgo.class.getName()));
        return userTableRuleConfig;
    }

}
```





##### Spring boot 配置

使用Spring Boot 的application.properties 来配置数据源和分片策略，要基于starter 依赖包模块。

**优点：**配置简单。

**缺点：**不能实现复杂的分片策略，不够灵活。



```yml
# MyBatis配置
mybatis.mapper-locations=classpath:mapper/*.xml
mybatis.config-location=classpath:mybatis-config.xml

# 数据源配置
sharding.jdbc.datasource.names=ds0,ds1
sharding.jdbc.datasource.ds0.type=com.alibaba.druid.pool.DruidDataSource
sharding.jdbc.datasource.ds0.driver-class-name=com.mysql.cj.jdbc.Driver
sharding.jdbc.datasource.ds0.url=jdbc:mysql://localhost:3306/ds0
sharding.jdbc.datasource.ds0.username=root
sharding.jdbc.datasource.ds0.password=123456

sharding.jdbc.datasource.ds1.type=com.alibaba.druid.pool.DruidDataSource
sharding.jdbc.datasource.ds1.driver-class-name=com.mysql.cj.jdbc.Driver
sharding.jdbc.datasource.ds1.url=jdbc:mysql://localhost:3306/ds1
sharding.jdbc.datasource.ds1.username=root
sharding.jdbc.datasource.ds1.password=123456

#sharding.jdbc.config.sharding.default-database-strategy.inline.sharding-column=user_id
#sharding.jdbc.config.sharding.default-database-strategy.inline.algorithm-expression=ds${user_id % 2}

# 分库算法 user_info，多库分表
# 单库内没有分表，注释了分表策略
sharding.jdbc.config.sharding.tables.user_info.actual-data-nodes=ds$->{0..1}.user_info
sharding.jdbc.config.sharding.tables.user_info.databaseStrategy.inline.shardingColumn=user_id
sharding.jdbc.config.sharding.tables.user_info.databaseStrategy.inline.algorithm-expression=ds${user_id % 2}
###sharding.jdbc.config.sharding.tables.user_info.databaseStrategy.standard.shardingColumn=user_id
###sharding.jdbc.config.sharding.tables.user_info.databaseStrategy.standard.preciseAlgorithmClassName=com.gupaoedu.config.DBShardAlgo
###sharding.jdbc.config.sharding.tables.user_info.tableStrategy.standard.shardingColumn=user_id
###sharding.jdbc.config.sharding.tables.user_info.tableStrategy.standard.preciseAlgorithmClassName=com.gupaoedu.config.TblPreShardAlgo
###sharding.jdbc.config.sharding.tables.user_info.tableStrategy.standard.rangeAlgorithmClassName=com.gupaoedu.config.TblRangeShardAlgo
##sharding.jdbc.config.sharding.tables.user_info.table-strategy.inline.sharding-column=user_id
##sharding.jdbc.config.sharding.tables.user_info.table-strategy.inline.algorithm-expression=user_info

# 分库算法 t_order 多库分表
sharding.jdbc.config.sharding.tables.t_order.databaseStrategy.inline.shardingColumn=order_id
sharding.jdbc.config.sharding.tables.t_order.databaseStrategy.inline.algorithm-expression=ds${order_id % 2}
sharding.jdbc.config.sharding.tables.t_order.actual-data-nodes=ds$->{0..1}.t_order

# 分库算法 t_order_item 多库分表
sharding.jdbc.config.sharding.tables.t_order_item.databaseStrategy.inline.shardingColumn=order_id
sharding.jdbc.config.sharding.tables.t_order_item.databaseStrategy.inline.algorithm-expression=ds${order_id % 2}
sharding.jdbc.config.sharding.tables.t_order_item.actual-data-nodes=ds$->{0..1}.t_order_item

# 绑定表规则列表，防止关联查询出现笛卡尔积
sharding.jdbc.config.sharding.binding-tables[0]=t_order,t_order_item

# 广播表（全局表，所有节点都存的表）
sharding.jdbc.config.sharding.broadcast-tables=t_config
```



##### yml配置

和第二种配置在一样，都是基于 starter 依赖包模块，只是配置在 yml 配置文件中。

```yml
sharding:
  jdbc:
    datasource:
      # 数据源
      names: ds0,ds1,ds2
      ds0:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://127.0.0.1:3306/shard0
        username: root
        password: 123456
      ds1:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://127.0.0.1:3306/shard1
        username: root
        password: 123456
      ds2:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://127.0.0.1:3306/shard2
        username: root
        password: 123456
    config:
      sharding:
        # 默认数据源，不分库分表到达这个数据源
        default-data-source-name: ds2
        #【默认分库策略】对user_id取模
        default-database-strategy:
          inline:
            sharding-column: user_id
            algorithm-expression: ds$->{user_id % 2}
        # 【分表策略】
        tables:
          # dictionary是广播表
          dictionary:
            key-generator-column-name: dictionary_id
            actual-data-nodes: ds$->{0..1}.dictionary
          # user表只分库不分表
          user:
            key-generator-column-name: user_id
            actual-data-nodes: ds$->{0..1}.user
          # order表分库分表
          order:
            key-generator-column-name: order_id
            actual-data-nodes: ds$->{0..1}.order$->{0..1}
            table-strategy:
              inline:
                sharding-column: order_id
                algorithm-expression: order$->{order_id%2}
          # order_item表分库分表
          order_item:
            key-generator-column-name: order_item_id
            actual-data-nodes: ds$->{0..1}.order_item$->{0..1}
            table-strategy:
              inline:
                sharding-column: order_id
                algorithm-expression: order_item$->{order_id%2}
      # 设置后会显示执行的sql语句和路由到那个节点
      props:
        sql.show: true
```



> 第二第三种实际优缺点一样，建议数据源配置在配置文件中，分片策略则用 Java 代码配置实现。

### 数据分片

Sharding-JDBC 中的分片策略有两个维度：

**分库（数据源分片）策略：**分库策略表示数据路由到的物理目标数据源

**分表策略：**分表分片策略表示数据被路由到的目标表。

分表策略是依赖于分库策略的，也就是说要先分库再分表，当然也可以不分库只分表。

跟Mycat 不一样，Sharding-JDBC 没有提供内置的分片算法，而是通过抽象成接口，让开发者自行实现，这样可以根据业务实际情况灵活地实现分片。



#### 分片策略

包含分片键和分片算法，分片算法是需要自定义的。可以用于分库，也可以用于分表。

Sharding-JDBC 提供了5 种分片策略，这些策略全部继承自ShardingStrategy。



##### 行表达式分片策略（InlineShardingStrategy）

对应InlineShardingStrategy 类。只支持单分片键，提供对=和IN 操作的支持。行内表达式的配置比较简单。

例如：

* `${begin..end}` 表示范围区间
* `${[unit1, unit2, unit_x]}` 表示枚举值
* `t_user_$->{u_id % 8} ` 表示t_user 表根据u_id 模8，而分成8 张表，表名称为t_user_0 到t_user_7。

行表达式中如果出现连续多个`${ expression }`或`$->{ expression }`表达式，整个表达式最终的结果将会根据每个子表达式的结果进行笛卡尔组合（即数据库和表的层面都是用了分片规则，最终所得表的个数为**分库数*分表数**，行表达式为 `${['db1', 'db2']}_table${1..3}`）。



##### 标准分片策略（StandardShardingStrategy）

标准分片策略只支持单分片键， 提供了提供 `PreciseShardingAlgorithm` 和`RangeShardingAlgorithm` 两个分片算法，分别对应于SQL 语句中的=, IN 和BETWEEN AND。

如果要使用标准分片策略，必须要实现PreciseShardingAlgorithm，用来处理=和IN 的分片。RangeShardingAlgorithm 是可选的。如果没有实现，SQL 语句会发到所有的数据节点上执行。



##### 复合分片策略（ComplexShardingStrategy）

可以支持等值查询和范围查询。比如：根据日期和ID 两个字段分片，每个月3 张表，先根据日期，再根据ID 取模。
复合分片策略支持多分片键，提供了ComplexKeysShardingAlgorithm，分片算法需要自己实现。



##### Hint 分片策略（HintShardingStrategy）

通过Hint 而非SQL 解析的方式分片的策略。有点类似于Mycat 的指定分片注解。



##### 不分片策略（NoneShardingStrategy）

不分片的策略。



#### 分片算法

Sharding-JDBC 目前提供4 种分片算法：

##### 精确分片算法（PreciseShardingAlgorithm）

用于处理使用单一键作为分片键的=与IN 进行分片的场景。需要配合StandardShardingStrategy 使用。



##### 范围分片算法（RangeShardingAlgorithm）

用于处理使用单一键作为分片键的BETWEEN AND 进行分片的场景。需要配StandardShardingStrategy 使用。如果不配置范围分片算法，范围查询默认会路由到所有节点。



##### 复合分片算法（ComplexKeysShardingAlgorithm）

用于处理使用多键作为分片键进行分片的场景，包含多个分片键的逻辑较复杂，需要应用开发者自行处理其中的复杂度。需要配合ComplexShardingStrategy 使用。



##### Hint 分片算法（HintShardingAlgorithm）

用于处理使用Hint 行分片的场景。需要配合HintShardingStrategy 使用。



#### 算法实现

所有的算法都需要实现对应的接口，实现doSharding()方法：

例如：

PreciseShardingAlgorithm，传入分片键，返回一个精确的分片（数据源名称）

```java
String doSharding(Collection<String> availableTargetNames, PreciseShardingValue<T> shardingValue);
```

RangeShardingAlgorithm，传入分片键，返回多个数据源名称

```java
Collection<String> doSharding(Collection<String> availableTargetNames, RangeShardingValue<T> shardingValue);
```

ComplexKeysShardingAlgorithm，传入多个分片键，返回多个数据源名称

```java
Collection<String> doSharding(Collection<String> availableTargetNames, Collection<ShardingValue> shardingValues);
```



### 分布式事务

Sharding-JDBC 默认支持多种分布式事务模型，如 XA 事务、柔性事务



#### 两阶段事务——XA 事务

添加依赖：

```xml
<dependency>
    <groupId>io.shardingsphere</groupId>
    <artifactId>sharding-transaction-2pc-xa</artifactId>
    <version>3.1.0</version>
</dependency>
```

XA 事务默认是用 atomikos 实现的。

在Service 类上加上注解，即可使用事务

```java
@ShardingTransactionType(TransactionType.XA)
@Transactional(rollbackFor = Exception.class)
```

> 其他事务类型：Local、BASE
>
> XAShardingTransactionManager—》XATransactionManager—》AtomikosTransactionManager



#### 柔性事务Saga

ShardingSphere 的柔性事务已通过第三方SPI 实现Saga 事务，Saga 引擎使用Servicecomb-Saga。
参考官方的这篇文章[《分布式事务在Sharding-Sphere 中的实现》](https://mp.weixin.qq.com/s?__biz=MzIwMzc4NDY4Mw==&mid=2247486704&idx=1&sn=edc76d838cbed006a107573732ba271b&chksm=96cb6474a1bced623e55db47e2d2fa5b7493eff5730bc3a6d0a5f1c4b08d2cc27fcccd61513e&mpshare=1&scene=1&srcid=1022xTpRYjxUbdUEZPgLpvV6&sharer_sharetime=1571725596151&sharer_shareid=035a2b61dca579f53be4eae95018f9cf&key=07d257d046da7c4bd15ab473b579736eb78a263ceaf74f267306f034ad9d436c49c55f92ae9d7ab24b1fd410a4de62b2ae1c6f968405882d59633162adc18476f01c6db048de3071e6c2549918a20836&ascene=1&uin=MTE5MTQzNjA0MA%3D%3D&devicetype=Windows+7&version=62070152&lang=zh_CN&pass_ticket=Q2%2FOQotkJiA%2F%2FFksN%2B2ivqC0PU2wLceaL8QDpYYehC5L5uxZ69kRygI53NXEXcdU)

```xml
<dependency>
    <groupId>io.shardingsphere</groupId>
    <artifactId>sharding-transaction-2pc-spi</artifactId>
    <version>3.1.0</version>
</dependency>
```



#### 柔性事务Seata
需要额外部署Seata-server 服务进行分支事务的协调。

官方的demo 中有一个例子：
https://github.com/apache/incubator-shardingsphere-example

````
incubator-shardingsphere-example-dev\sharding-jdbc-example\transaction-example\transaction-base-seata-raw-jdbc-example
````

### 全局id

Sharding-JDBC对全局id默认生成了一个18 位的ID。使用方式如下：

* yml 配置：

  使用key-generator-column-name 配置，

* Properties 配置则配置如下：

  ```properties
  sharding.jdbc.config.sharding.default-key-generator-class-name=
  sharding.jdbc.config.sharding.tables.t_order.keyGeneratorColumnName=
  sharding.jdbc.config.sharding.tables.t_order.keyGeneratorClassName=
  ```

* Java 代码配置

  ```java
  tableRuleConfig.setKeyGeneratorColumnName("order_id");
  tableRuleConfig.setKeyGeneratorClass("io.shardingsphere.core.keygen.DefaultKeyGenerator");
  ```

> * keyGeneratorColumnName：指定需要生成ID 的列
> * KeyGenerotorClass：指定生成器类，默认是DefaultKeyGenerator.java，里面使用了雪花算法（生成了一个18 位的ID）。

### 与Mycat对比

|            | Sharding-JDBC            | Mycat                               |
| ---------- | ------------------------ | ----------------------------------- |
| 工作层面   | JDBC 协议                | MySQL 协议/JDBC 协议                |
| 运行方式   | Jar 包，客户端           | 独立服务，服务端                    |
| 开发方式   | 代码/配置改动            | 连接地址（数据源）                  |
| 运维方式   | 无                       | 管理独立服务，运维成本高            |
| 性能       | 多线程并发按操作，性能高 | 独立服务+网络开销，存在性能损失风险 |
| 功能范围   | 协议层面                 | 包括分布式事务、数据迁移等          |
| 适用操作   | OLTP                     | OLTP+OLAP                           |
| 支持数据库 | 基于JDBC 协议的数据库    | MySQL 和其他支持JDBC 协议的数据库   |
| 支持语言   | Java 项目中使用          | 支持JDBC 协议的语言                 |

从易用性和功能完善的角度来说，Mycat 似乎比Sharding-JDBC 要好，因为有现成的分片规则，也提供了4 种ID 生成方式，通过注解可以支持高级功能，比如跨库关联查询。

建议：小型项目，分片规则简单的项目可以用Sharding-JDBC。大型项目，可以用Mycat。