# CPU 内存模型

对于一台计算机最核心的组件是CPU、内存、和I/O设备（磁盘）。

三者处理速度有差异：CPU > 内存 > I/O设备（磁盘）

CPU处理速度远大于内存和I/O设备，假设CPU要对I/O设备中的某个数据进行修改，其步骤如下：

1. 到I/O设备获取到数据，将数据读取到内存
2. CPU操作内存中的数据修改
3. CPU将内存中的数据写回到I/O设备中

可以看到在I/O处理的过程中，CPU需要等待，导致没有充分利用CPU的性能。

因此为了更加充分利用CPU资源，做了以下改进：

## 高速缓存

![CPU高速缓存](C:\Users\63190\Desktop\pics\CPU高速缓存.png)

高速缓存是一种读写速度尽可能接近CPU运算速度的高速缓存，用于减少内存和处理器之间运算速度差异。

CPU会把所需要的数据从内存中复制到高速缓存中，从而加快运算，运算完后再将缓存同步到内存中。

## 缓存一致性

在多CPU中，每个线程都运行在不同的CPU内，因此每个线程都拥有自己的高速缓存。此时如果多个线程共享同一个数据，就会出现该数据存储在多个高速缓存中，多个线程各自对自己高速缓存中的该数据进行操作，使得该数据在高速缓存中值不一样（即缓存不一致）。

为了防止缓存不一致，CPU提供以下方案：

### 总线锁

CPU对共享的数据（内存）进行访问时，首先在总线上加上锁（LOCK指令）。在该CPU释放之前，其他CPU都无法访问该共享的数据。

缺点：开销很大，每次操作都会加锁，其他CPU要等待其释放才能操作。

### 缓存锁

缓存锁基于缓存一致性协议(如MESI)实现。



#### MESI

MESI 表示缓存数据的四种状态：

1. M（Modify）：表示共享数据只缓存在当前 CPU 缓存中，并且是被修改状态。（即该数据在缓存中的值和主内存中的值不一样）
2. E（Exclusive）： 表示缓存的独占状态，数据只缓存在当前CPU 缓存中，并且没有被修改。
3. S（Shared）： 表示数据可能被多个 CPU 缓存，并且数据在各个缓存中的值和主内存中的值一样。
4. I（Invalid）：表示缓存已经失效。

在MESI协议中，每个缓存的控制器不仅知道自己的读写操作，而且也监听者其他缓存的读写操作。

> 如果一个数据只被一个CPU缓存，则该缓存数据为E状态。该状态的缓存可以读和写。
>
> 如果一个数据被多个CPU缓存（值没有变化，和主内存一致），则该该缓存数据为S状态。
>
> 如果某个缓存中的S状态数据要修改，其状态会变成M，并且会使其他缓存中的该数据状态变为I。
>
> M 状态可读可写，I状态只能从新在主内存同步该值。



MESI 优化：

当缓存中的S状态数据要修改时，首先要通知其他缓存了该数据的CPU该缓存数据要失效，此时修改的CPU会阻塞等待其他 CPU 返回失效处理响应后才修改数据值。为了避免阻塞等待，在 CPU 中加入了 Store Bufferes 。

对于共享数据修改时（写入操作），首先会写入到 Store Bufferes 中，然后通知其他缓存了该数据的CPU缓存失效，此时修改的 CPU 不会阻塞等待，而是去执行其他指令。当其他所有 CPU 返回了失效处理响应后，才会将  Store Bufferes  的数据写到缓存中，最后将缓存同步到主内存中。

> 读取数据值会优先从 Store Bufferes 中读取，如果没有才到缓存中读取。

实际上优化后会新的问题：

由于引入 Store Bufferes，导致写入到缓存是异步的（即同步到主内存是不确定的），CPU指令重排序后可能会导致可见性问题。如下：

```java
共享数据：
int val = 1;
cpu0执行如下{
    val = 10; // 此时由S->M,异步执行
    isFinish = true;
}
// 假设isFinish是E状态，那么实际CPU重排序后可能导致isFinish会同步到主内存，val还未同步到主内存
// 此时CPU1执行：
cpu1执行如下{
    if (isFinish) { // 由于CPU0已经同步到主内存，因此CPU1读取isFinish的值为true
        assert val == 10; // 由于CPU0还未val值同步到主内存，因此为false
    }
}
```

上面例子可以看到由于异步写入，共享数据修改值同步到主内存不确定性，导致其他 CPU 无法读取最新值而产生可见性问题。



## 可见性

可见性问题实际就是缓存一致性的问题。共享数据在各个缓存中可能存在不同的值。

导致可见性问题原因：

1. 缓存中数据修改，其他缓存并不知道。
2. 通过缓存一致性协议可以解决上面一点，让其他缓存的数据失效重新从主内存去获取最新值。但是还是会存在缓存中修改的值不一定会立刻同步到主内存中。（可能是CPU执行乱序问题）



## 内存屏障

由于硬件层面不能提前知道软件层面对于共享数据前后依赖需求（如上面举的例子，无法知道共享数据 val 一定要在 isFinish 值同步前同步到主内存中这种依赖关系），因此硬件层面提供了 memory barrier （内存屏障）指令。

内存屏障指令可以理解为将 CPU Store Bufferes 写入到内存中的指令。由软件层面自身决定在什么地方插入内存屏障，使得其他 CPU 能够在该内存屏障指令后读到共享数据最新的值。

X86的 memory barrier 指令包括：

* sfence（写屏障）：通知CPU在写屏障指令前将 store buffers 中的数据同步到主内存中。
* Ifence（读屏障）：CPU读操作要在读屏障后处理，配合写屏障，使得写屏障之前的内存更新对于读屏障之后的读操作是可见的。
* mfence（全屏障）：实际就是度写屏障配合使用。

通过读写屏障解决上述换群一致性问题：

```java
共享数据：
int val = 1;
cpu0执行如下{
    val = 10;
    sfence(); // 加入写屏障，确保共享数据val会同步到主内存中，避免重排序
    isFinish = true;
}
// 此时CPU1执行：
cpu1执行如下{
    if (isFinish) {
        // 加入读屏障，确保读操作会在写屏障后执行（store buffers 数据同步到主内存后）
        lfence();
        assert val == 10;
    }
}
```



# 虚拟内存（Vitual Memory）

名词解释：主存：内存；辅存：磁盘（硬盘）
计算机主存（内存）可看作一个由 M 个连续的字节大小的单元组成的数组，每个字节有一个唯一的地址，这个地址叫做物理地址（PA）。

物理寻址：通过主存字节对应的物理地址直接访问主存。

物理寻址弊端：

1. 在多用户多任务操作系统中，所有的进程共享主存，如果每个进程都独占一块物理地址空间，主存很快就会被用完。我们希望在不同的时刻，不同的进程可以共用同一块物理地址空间。
2. 如果所有进程都是直接访问物理内存，那么一个进程就可以修改其他进程的内存数据，导致物理地址空间被破坏，程序运行就会出现异常。



为了解决这些问题，通过在CPU 和主存之间增加一个中间层。CPU 不再使用物理地址访问主存，而是访问一个虚拟地址，由这个中间层把地址转换成物理地址，最终获得主存的数据。这个中间层就叫做虚拟存储器（Virtual Memory）

![虚拟内存](C:\Users\63190\Desktop\pics\虚拟内存.jpg)

在每一个进程开始创建的时候，都会分配一段虚拟地址，然后通过虚拟地址和物理地址的映射来获取真实数据，这样进程就不会直接接触到物理地址，甚至不知道自己调用的哪块物理地址的数据。

目前，大多数操作系统都使用了虚拟内存，如Windows 系统的虚拟内存、Linux 系统的交换空间等等。Windows 的虚拟内存（pagefile.sys）是磁盘空间的一部分。

在32 位的系统上，虚拟地址空间大小是2^32bit=4G。在64 位系统上，最大虚拟地址空间大小理论上是2^64bit=1024*1014TB=1024PB=16EB，实际上由于用不上这么大的空间，如 linux 就会用 48 位表示虚拟空间地址就足够（2^48bit=256T）。

总结：虚拟内存，可以提供更大的地址空间，并且地址空间是连续的，使得程序编写、链接更加简单。并且可以对物理内存进行隔离，不同的进程操作互不影响。还可以通过把同一块物理内存映射到不同的虚拟地址空间实现内存共享。

## 用户空间和内核空间

为了避免用户进程直接操作内核，保证内核安全，操作系统将虚拟内存划分为两部分，一部分是内核空间（Kernel-space），一部分是用户空间（User-space）。

![用户空间和内核空间](C:\Users\63190\Desktop\pics\用户空间和内核空间.jpg)

内核空间：操作系统的核心，中存放的是内核代码和数据，独立于普通的应用程序，可以访问受保护的内存空间，可以访问底层硬件设备的权限。提供系统访问接口给用户空间调用，从而隔离了用户程序直接访问系统底层资源。保证内核安全。

用户空间：存放的是用户程序的代码和数据。可以通过内核空间提供的接口间接访问系统资源。

> 内核空间和用户空间大小大概是1:3

## 进程状态切换

进程运行在 CPU 上。

用户态：进程在用户空间运行时叫作用户态。此时可以执行用户程序代码，并对程序中的数据进行简单的运算操作，但不能调用系统资源（因为系统资源需要通过内核空间提供的系统接口访问）。

内核态：进程在内核空间运行时叫作内核态。进程从用户空间调用内核空间提供的系统接口，使得进程切换到了内核空间，并执行相应的系统资源访问。

进程状态切换：

1. 用户代码访问系统资源。此时会从用户态转换成内核态。

2. 进程挂起和恢复。

   在常见的多进程模型，理论上有多少个 CPU 就可以并行（同一时间执行）的运行多少个进程。

   但实际上运行的进程远多于 CPU 数量，此时为了让多个进程都可以被执行，操作系统通过将 CPU 资源分成时间片，分给了多个进程，进程获取到 CPU 时间片就可以执行，使得在宏观上多个进程是并发执行（因为切换时间很短）。

   对于没获得 CPU 时间片的进程会被内核挂起，直到下次获得 CPU 时间片才会重新执行。为了让 CPU 能够继续执行被挂起的进程，就需要标记任务加载位置和代码执行位置，这些信息叫作CPU 的上下文，会存放在内核空间中。

   当进程在用户态时被挂起了，会切换到内核态存储相应的 CPU 上下文，以便于下次获取 CPU 时间片后继续执行。

   当被挂起进程获取 CPU 时间片后，会从内核空间中存储的 CPU 上下文信息设置到 CPU 的寄存器和程序计数器(Program Counter)上，此时从内核态切换回用户态继续执行用户代码。

> 可以看到 CPU 上下文的切换（挂起后存储上下文和上下文恢复执行）的操作是十分消耗资源的，因此在开发时候不一定越多进程越好（多线程同理），要考虑CPU 上下文切换带来的损耗。



## 进程阻塞

正在运行的进程由于提出系统服务请求（如I/O 操作），但因为某种原因未得到操作系统的立即响应，该进程只能把自己变成阻塞状态（进程在阻塞状态不占用CPU 资源）。当等待相应的事件出现后会对阻塞进程进行唤醒，并切进程重新获得 CPU 时间片后即可重新运行。



## 文件描述符FD

Linux 系统将系统所有资源都当做文件处理（包括硬件设备），而这些文件对象通过文件描述符FD来标识。

文件描述符（File Descriptor）：是内核为了高效管理已被打开的文件所创建的索引，是一个简单的非负整数，用于指向被进程打开的文件。

因此进程在执行 I/O 操作的系统调用，实际上是通过文件描述符去访问到系统设备（文件）；

Linux 系统里面有三个标准文件描述符：

0：标准输入（如键盘）；1：标准输出（如显示器）；2：标准错误输出（如显示器）。



## I/O 读操作案例

当进程执行用户程序，程序需要读取磁盘信息时（I/O 读操作），会调用内核空间提供的 read 系统接口通过文件描述符FD索引到对应文件（磁盘）获取数据，如果数据在内核空间已经存在，则直接将该数据拷贝到用户进程的页内存中。如果不存在，则首先会到磁盘中加载数据到内核缓冲区中，然后再拷贝到用户进程的页内存中。

可以看到在一次I/O操作中会存在两次进程状态切换（第一次用户态切换到内核态进行获取数据，第二次内核态切换到用户态将数据拷贝到用户进程的页内存中然后继续执行）

![IO读操作](C:\Users\63190\Desktop\pics\IO读操作.jpg)

## I/O 阻塞

当使用 read 对某个文件描述符进行过读取时，如果 FD 不可读（数据没有从磁盘拷贝到内核空间），此时会阻塞等待，无法继续执行其他命令。

**读操作阻塞**

1. 从设备复制数据到内核缓冲区。
2. 从内核缓冲区拷贝到用户空间。

**写操作阻塞**

1. 数据写入到设备中。

可以看到直到数据读写完毕，内核返回结果后，用户进程才解除 block 的状态。这种 I/O 执行模型称为同步阻塞。



### 阻塞解决方案

**非阻塞 IO**

非阻塞 I/O 又称为 NIO，通过请求方不断的去轮询数据是否从硬件设备拷贝到内核空间，如果没有则请求方可以继续处理其他事情，并之后不断继续轮询。

缺点：

1. 由于通过请求方不断轮询数据是否拷贝到内核空间，因此会造成一定的消耗而且结果会有延迟。
2. 数据准备好在内核空间后，请求方还是需要阻塞的将数据从内核空间拷贝到用户空间。



**伪异步 IO**

将任务分给多个线程执行，使得某些任务执行中被阻塞，也不影响其他任务执行。

缺点：线程的创建销毁同样会消耗系统资源，而且高并发下可能存在大量的上下文切换，也会消耗系统资源。

由于被阻塞的任务还是会阻塞等待，因此这种 I/O 执行模型也成为了伪异步 IO，并没有本质上解决 IO 阻塞问题，而是从整体上不会阻塞多个任务。

优化：可以采用线程池，避免无限创建线程。



**异步 IO**

通过事件通知的方式，请求方不需要等待 I/O 结果，当 I/O 处理完后会通知请求方，请求方收到通知后再发起请求获取 I/O 结果。



## I/O 多路复用

I/O：指的是网络 IO。

多路：指的是多个 Socket 请求。

复用：指的是单个线程可以同时接收处理多个请求。

情景：传统的网络 I/O 处理中，客户端程序发送 I/O 请求（Socket），服务端程序（单线程，I/O复用处理程序 processor）对于收到的 I/O 请求请求内核中的数据，此时服务端程序会阻塞等待内核返回数据，此时服务端程序在阻塞等待过程中，无法响应其他客户端的 I/O 请求。

为了解决上述的问题，提出了 I/O 多路复用，原理如下：

1. 服务端程序将多个客户端程序发起的 Socket 请求（会有不同的事件类型）对应转换成文件描述符存储到队列中，此时客户端程序会阻塞等待 Socket 请求结果。
2. 服务端程序调用指定多路复用模型器接口，将队列的 fd 传到内核中，由内核根据指定的多路复用模型从队列中的fd找到其中一个数据准备好的 fd，并通知服务端程序可以获取数据。
3. 服务端程序收到通知后，再请求 Socket 数据，将内核准备好的数据拷贝到服务端程序的内存中，拷贝过程中服务端程序是阻塞等待，直到拷贝完数据。
4. 在第二步的过程中，服务端程序依旧可以接收其他客户端传来的 Socket 请求，并对应转换成fd存储到队列中。

以下是 Select 多路复用模型流程图：

![IO 多路复用Select模型](C:\Users\63190\Desktop\pics\IO 多路复用Select模型.jpg)

### 多路复用模型

每个操作系统对于多路复用模型都有不同的实现，但 select 多路复用模型是基本模型。而不同的操作系统会有更高效率的多路复用模型实现，如 Linux 提供 epoll 模型，Solaris 提供 poll 模型。

在上面第二步中实际上会指定多路复用模型，如调用内核提供的 select 接口，就会使用 select 模型。

不同的多路复用模型实际区别在于如何获取到那个fd数据准备好了。

**Select模型**

select 模型会对内核传入的 fd 队列进行轮询查找其中一个数据准备好的fd。

select 模型缺点：

1. 由于服务端程序轮询调用 select。因此进程状态会从用户态切换成内核态，造成资源消耗。
2. 在调用 select 过程中需要把 fd 队列从用户空间拷贝到内核空间，随着队列的fd增加消耗也增加。

3. select 模型是通过数组存储 fd，因此支持的文件描述符数量有限，默认是1024个，导致最大连接数限制。

**Poll模型**

Poll 模型工作和 Select 模型工作原理基本一致，区别在于：

1. Poll 模型中使用链表的结构存储 fd，因此不存在连接数限制。
2. Poll 模型执行过程中会将所有数据准备就绪的 fd 放到等待队列中，不像 Select 一样每次最多返回一个。已经准备好，但没有被处理的 fd，会在下次 poll 中继续被返回。

**epoll模型**

epoll 提供了三个函数：

- `int epoll_create(int size)`：建立一個 epoll 对象，并传回它的id。
- `int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)`：事件注册函数，将需要监听的事件和需要监听的fd交给epoll对象。
- `int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout)`：等待注册的事件被触发或者timeout发生。

epoll 好处：

1. epoll没有fd数量限制。

   epoll 将每一个 fd 和 epoll 对象关联（epoll 对象监听 fd 事件，并指定回调函数），因此最大连接数实际为能打开的 fd 数量有关。

2. epoll 不需要多次拷贝 fd 到内核。

   epoll 在 epoll_ctl 注册事件时就将 fd 拷贝到内核中，不需要每次查找是否有 fd 准备好数据时都拷贝一遍到内核中。

- epoll 不需要主动轮询 fd 是否准备就绪，也不会随着 fd 数量越多，执行效率会变慢。

  select 和 poll 都是基于主动同步轮询机制，在获取准备好的 fd 时会遍历每一个人fd，因此会存在随着 fd 数量越多，判断时间越长效率越低。而 epoll 是基于事件驱动机制，通过 os 内核实现伪异步的模式，当有 fd 准备好时，会将 fd 放入到等待队列中（epoll 内部维护的队列），无需主动去轮询（阻塞等待轮询结果）。在调用 epoll_wait 获取是否有 fd 准备好时，只需判断等待队列是否有 fd，如果有则调用这些 fd 的回调函数。

### 总结

从总体上来说还是存在阻塞等待：

1. 客户端程序会阻塞等待服务端程序的 Socket 结果
2. 服务端程序如使用 Select、Poll 多路复用模型查找数据准备好的fd过程也是阻塞等待结果。
3. 服务端程序去获取已经准备好数据的fd时，也是阻塞等待数据拷贝。



但可以看到使用 I/O 多路复用有以下好处：

1. 服务端程序可以接收多个客户端的Socket请求，实现的单线程也可以处理多个请求（复用）。
2. 服务端程序不需要主动监听（如轮询）fd数据是否准备好，而是由内核去监听并通知服务端程序获取，是实现第一点的基础。



多路复用模型选择：

select 模型是最基础的模型，不同的操作系统会提供不同的多路复用模型。

在处理大量连接时，select 模型和 poll 模型由于主动轮询的机制，导致随着连接数越多，效率越低。如果使用 linux 系统，可以考虑改成 epoll 模型。



I/O 多路复用使用场景：对于多连接的情景下，使用 I/O 多路复用能够很好地提升性能，但如果单链接的情况下就没有必要使用。